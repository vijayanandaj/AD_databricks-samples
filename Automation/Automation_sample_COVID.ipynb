{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e494c43e-fe70-4af1-b181-ea305b21f4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /databricks-datasets/COVID/covid-19-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a2a4ae54-8118-40d2-9b01-05183cd8e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Look at what’s already on DBFS\n",
    "dbutils.fs.ls('/databricks-datasets/COVID/covid-19-data/')\n",
    "\n",
    "# 2) Define  Bronze path\n",
    "bronze = \"/tmp/bronze/covid_nyt\"\n",
    "\n",
    "# Drop & recreate\n",
    "dbutils.fs.rm(bronze, recurse=True)\n",
    "dbutils.fs.mkdirs(bronze)\n",
    "\n",
    "# 3) Ingest all the CSVs into a single Bronze Delta\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "(\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(\"dbfs:/databricks-datasets/COVID/covid-19-data/*.csv\")\n",
    "         .write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .save(bronze)\n",
    ")\n",
    "\n",
    "# 4) Verify it worked\n",
    "print(\"Bronze row count:\", spark.read.format(\"delta\").load(bronze).count())\n",
    "display(spark.read.format(\"delta\").load(bronze).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4912151b-4bc6-4856-980a-f51bec603043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display( dbutils.fs.ls(\"/tmp/bronze/covid_nyt\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "215fca34-0557-4c84-8709-3f946a5cf9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--register bronze table into the Hive meta store \n",
    "USE CATALOG spark_catalog;    -- the legacy HMS catalog\n",
    "USE SCHEMA default;           -- or whichever HM DB you prefer\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bronze_covid_nyt\n",
    "USING DELTA\n",
    "LOCATION 'dbfs:/tmp/bronze/covid_nyt';\n",
    "\n",
    "\n",
    "SHOW TABLES;\n",
    "\n",
    "--%sql\n",
    "--SHOW DATABASES;\n",
    "\n",
    "--%sql\n",
    "--SHOW TABLES IN exampleDB;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1d299f3d-40c8-4877-80a0-86c91b1d095b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#Create the look up table /.parquet\n",
    "from pyspark.sql.functions import col\n",
    "# 1. Read the full dataset (fact table)\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"dbfs:/databricks-datasets/COVID/covid-19-data/us-states.csv\")\n",
    "\n",
    "# 2. Extract distinct state→FIPS pairs\n",
    "lookup_df = (df\n",
    "  .select(col(\"state\").alias(\"state\"),\n",
    "          col(\"fips\").cast(\"string\").alias(\"state_fips\"))\n",
    "  .where(col(\"fips\").isNotNull())\n",
    "  .distinct())\n",
    "\n",
    "# 3. Write out just your lookup table\n",
    "lookup_df.write.mode(\"overwrite\").parquet(\"dbfs:/tmp/state_fips_lookup.parquet\")\n",
    "\n",
    "# Read it back\n",
    "df2 = spark.read.parquet(\"dbfs:/tmp/state_fips_lookup.parquet\")\n",
    "\n",
    "# Console view\n",
    "df2.show(truncate=False)\n",
    "\n",
    "# Databricks table view\n",
    "display(df2)\n",
    "\n",
    "display(df2.orderBy(\"state_fips\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b45b6c05-59f4-4ba6-8586-a8d1370489de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Create silver layer\n",
    "-- Switch into your HMS catalog\n",
    "USE CATALOG spark_catalog;\n",
    "USE SCHEMA default;\n",
    "\n",
    "-- 1) Clean & cast\n",
    "CREATE OR REPLACE TABLE silver_covid_nyt_clean\n",
    "USING DELTA\n",
    "LOCATION 'dbfs:/tmp/silver/covid_nyt_clean'\n",
    "AS\n",
    "SELECT\n",
    "  CAST(date   AS DATE)   AS event_date,\n",
    "  state                  AS region,\n",
    "  CAST(cases  AS INT)    AS cases,\n",
    "  CAST(deaths AS INT)    AS deaths\n",
    "FROM default.bronze_covid_nyt\n",
    "WHERE date        IS NOT NULL\n",
    "  AND state       RLIKE '^[A-Za-z ]+$' --state RLIKE '^[A-Za-z ]+$' drops every row where the region was purely numeric (e.g. “50”, “0”, “12382” …) or had any non-letter characters—so all those garbage region codes vanish.\n",
    "  AND cases       IS NOT NULL\n",
    "  AND deaths      IS NOT NULL;\n",
    "--date IS NOT NULL, cases IS NOT NULL, deaths IS NOT NULL toss any records missing a date or a count.\n",
    "\n",
    "-- 2) Deduplicate\n",
    "CREATE OR REPLACE TABLE silver_covid_nyt_dedup\n",
    "USING DELTA\n",
    "LOCATION 'dbfs:/tmp/silver/covid_nyt_dedup'\n",
    "AS\n",
    "SELECT DISTINCT *\n",
    "FROM silver_covid_nyt_clean;\n",
    "\n",
    "-- 3) Enrich with FIPS (only once!)\n",
    "CREATE OR REPLACE TABLE silver_covid_nyt\n",
    "USING DELTA\n",
    "LOCATION 'dbfs:/tmp/silver/covid_nyt'\n",
    "AS\n",
    "SELECT\n",
    "  d.event_date,\n",
    "  d.region,\n",
    "  d.cases,\n",
    "  d.deaths,\n",
    "  l.state_fips\n",
    "FROM silver_covid_nyt_dedup AS d\n",
    "LEFT JOIN parquet.`dbfs:/tmp/state_fips_lookup.parquet` AS l\n",
    "  ON d.region = l.state\n",
    "WHERE l.state_fips IS NOT NULL;\n",
    "\n",
    "-- Verify\n",
    "SHOW TABLES IN default;\n",
    "SELECT * FROM silver_covid_nyt LIMIT 5;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8d11045-7731-4b2d-8f79-a31826ed7ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Gold layer the key business metrics from your silver data. Typically you’ll want:\n",
    "#State‐level cumulative summary (total cases & deaths to date)\n",
    "#Daily metrics (new cases/deaths & 7-day rolling averages)\n",
    "\n",
    "from pyspark.sql.functions import col, sum as _sum, max as _max, avg as _avg\n",
    "from pyspark.sql.window import Window\n",
    "# 1a. Read silver\n",
    "silver = spark.read.format(\"delta\").table(\"default.silver_covid_nyt\")\n",
    "\n",
    "# 1b. Build cumulative summary\n",
    "# ---------------------------------------------------\n",
    "# Gold Table 1: State‐level Cumulative Summary\n",
    "# ---------------------------------------------------\n",
    "cumulative = (\n",
    "    silver\n",
    "     .groupBy(\"region\", \"state_fips\")\n",
    "     .agg(\n",
    "         _max(\"event_date\").alias(\"last_report_date\"),\n",
    "         _sum(\"cases\").alias(\"total_cases\"),\n",
    "         _sum(\"deaths\").alias(\"total_deaths\")\n",
    "     )\n",
    ")\n",
    "\n",
    "# 1c. Write out as gold\n",
    "# Write & register\n",
    "cumulative.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/tmp/gold/covid_state_summary\") \\\n",
    "    .saveAsTable(\"default.gold_covid_state_summary\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Gold Table 2: Daily Metrics with 7-Day Rolling Avg\n",
    "# ---------------------------------------------------\n",
    "# Define a 7-day window per region\n",
    "w = Window.partitionBy(\"region\").orderBy(\"event_date\").rowsBetween(-6, 0)\n",
    "\n",
    "daily_metrics = (\n",
    "    silver\n",
    "    .groupBy(\"event_date\", \"region\", \"state_fips\")\n",
    "    .agg(\n",
    "        _sum(\"cases\").alias(\"new_cases\"),\n",
    "        _sum(\"deaths\").alias(\"new_deaths\")\n",
    "    )\n",
    "    .withColumn(\"avg7_new_cases\",  _avg(\"new_cases\").over(w))\n",
    "    .withColumn(\"avg7_new_deaths\", _avg(\"new_deaths\").over(w))\n",
    ")\n",
    "\n",
    "# Write & register\n",
    "daily_metrics.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"dbfs:/tmp/gold/covid_daily_metrics\") \\\n",
    "    .saveAsTable(\"default.gold_covid_daily_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8e439d1e-a4b4-46f9-9222-5d636b5887f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Queries on Gold layer\n",
    "-- List all rows (preview)\n",
    "--gold_covid_state_summary should show one row per state with its FIPS code, the last report date, total cases and total deaths.\n",
    "SELECT * \n",
    "FROM default.gold_covid_state_summary\n",
    "LIMIT 10;\n",
    "\n",
    "SELECT * \n",
    "FROM default.gold_covid_daily_metrics\n",
    "LIMIT 10;\n",
    "\n",
    "%sql \n",
    "SELECT * \n",
    "FROM default.silver_covid_nyt\n",
    "LIMIT 10;\n",
    "\n",
    "%sql\n",
    "--Latest snapshot across all states\n",
    "SELECT\n",
    "  d.region,\n",
    "  d.event_date,\n",
    "  d.new_cases,\n",
    "  d.avg7_new_cases,\n",
    "  s.total_cases,\n",
    "  s.total_deaths\n",
    "FROM default.gold_covid_daily_metrics AS d\n",
    "JOIN default.gold_covid_state_summary AS s\n",
    "  ON d.region = s.region\n",
    "WHERE d.event_date = (SELECT MAX(event_date) FROM default.gold_covid_daily_metrics)\n",
    "ORDER BY d.new_cases DESC\n",
    "LIMIT 10;\n",
    "\n",
    "--Top 5 states by peak 7-day rolling average\n",
    "%sql\n",
    "SELECT\n",
    "  region,\n",
    "  MAX(avg7_new_cases) AS peak_avg7_cases\n",
    "FROM default.gold_covid_daily_metrics\n",
    "GROUP BY region\n",
    "ORDER BY peak_avg7_cases DESC\n",
    "LIMIT 5;\n",
    "\n",
    "--Trend line for a specific state\n",
    "\n",
    "%sql\n",
    "SELECT\n",
    "  event_date,\n",
    "  new_cases,\n",
    "  avg7_new_cases\n",
    "FROM default.gold_covid_daily_metrics\n",
    "WHERE region = 'New York'\n",
    "ORDER BY event_date;\n",
    "\n",
    "\n",
    "--Percentage of deaths on the latest date\n",
    "\n",
    "%sql\n",
    "WITH latest AS (\n",
    "  SELECT * \n",
    "  FROM default.gold_covid_daily_metrics\n",
    "  WHERE event_date = (SELECT MAX(event_date) FROM default.gold_covid_daily_metrics)\n",
    ")\n",
    "SELECT\n",
    "  l.region,\n",
    "  l.new_deaths,\n",
    "  s.total_deaths,\n",
    "  ROUND(100.0 * l.new_deaths / s.total_deaths, 2) AS pct_new_vs_total_deaths\n",
    "FROM latest l\n",
    "JOIN default.gold_covid_state_summary s\n",
    "  ON l.region = s.region\n",
    "ORDER BY pct_new_vs_total_deaths DESC\n",
    "LIMIT 10;\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5126676499304179,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Automation_sample_COVID",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
