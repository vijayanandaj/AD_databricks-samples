{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40791d78-9dd6-42b5-8d44-987fcc992794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold Layer Aggregation for telemetry1 events\n",
    "# Spark 4.0.0, serverless cluster compatible\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    date_format, col, countDistinct, count, avg\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Paths for Silver and Gold layers\n",
    "silver_path = \"dbfs:/tmp/silver/telemetry1/\"\n",
    "gold_path   = \"dbfs:/tmp/gold/telemetry1/\"\n",
    "\n",
    "# 0) Ensure Gold folder exists\n",
    "dbutils.fs.mkdirs(gold_path)\n",
    "\n",
    "# 1) Read Silver Delta\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# 2) Add event_date for daily aggregations\n",
    "silver_df = silver_df.withColumn(\n",
    "    \"event_date\", date_format(col(\"event_timestamp\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# 3) Gold Table 1: Daily App Metrics\n",
    "app_daily = (\n",
    "    silver_df\n",
    "      .groupBy(\"event_date\", \"app_name_norm\")\n",
    "      .agg(\n",
    "        count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"user_id\").alias(\"daily_active_users\"),\n",
    "        avg(\"is_purchase\").alias(\"purchase_rate\")\n",
    "      )\n",
    ")\n",
    "app_daily.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .save(f\"{gold_path}/app_daily_metrics\")\n",
    "\n",
    "# 4) Gold Table 2: Daily Event-Type Distribution\n",
    "event_daily = (\n",
    "    silver_df\n",
    "      .groupBy(\"event_date\", \"event_type_norm\")\n",
    "      .agg(\n",
    "        count(\"*\").alias(\"event_count\")\n",
    "      )\n",
    ")\n",
    "event_daily.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .save(f\"{gold_path}/event_daily_metrics\")\n",
    "\n",
    "# 5) Gold Table 3: Daily Region Metrics\n",
    "region_daily = (\n",
    "    silver_df\n",
    "      .groupBy(\"event_date\", \"region\")\n",
    "      .agg(\n",
    "        count(\"*\").alias(\"region_events\"),\n",
    "        countDistinct(\"user_id\").alias(\"region_active_users\")\n",
    "      )\n",
    ")\n",
    "region_daily.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .save(f\"{gold_path}/region_daily_metrics\")\n",
    "\n",
    "# 6) Optimize all Gold tables for performance\n",
    "for sub in [\"app_daily_metrics\", \"event_daily_metrics\", \"region_daily_metrics\"]:\n",
    "    path = f\"{gold_path}/{sub}\"\n",
    "    DeltaTable.forPath(spark, path).optimize()\n",
    "\n",
    "# 7) Sanity-check: print counts\n",
    "print(\"App Daily partitions:\", spark.read.format(\"delta\").load(f\"{gold_path}/app_daily_metrics\").count())\n",
    "print(\"Event Daily partitions:\", spark.read.format(\"delta\").load(f\"{gold_path}/event_daily_metrics\").count())\n",
    "print(\"Region Daily partitions:\", spark.read.format(\"delta\").load(f\"{gold_path}/region_daily_metrics\").count())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Telelemetry1_Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
