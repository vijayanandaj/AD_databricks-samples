{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4d19341e-393d-4acb-9675-bbf2335eea3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 03 - Unified batch and Stream \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# CLEAN UP\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/delta_demo_batch\", recurse=True)\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/delta_demo_stream\", recurse=True)\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/delta_demo_stream_ckpt\", recurse=True)\n",
    "\n",
    "# 1) BATCH WRITE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# write IDs 0â€“9 in a single batch, *partitioned* on mod2 so you see two folders\n",
    "batch_path = \"dbfs:/FileStore/delta_demo_batch\"\n",
    "\n",
    "spark.range(0, 10) \\\n",
    "     .withColumn(\"mod2\", col(\"id\") % 2) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .partitionBy(\"mod2\") \\\n",
    "     .save(batch_path)\n",
    "\n",
    "print(\"ğŸ“¦ Batch write â€” directory listing:\")\n",
    "display(dbutils.fs.ls(batch_path))           \n",
    "\n",
    "print(\"ğŸ” Inside mod2=0 folder:\")\n",
    "display(dbutils.fs.ls(f\"{batch_path}/mod2=0\"))  \n",
    "print(\"ğŸ” Inside mod2=1 folder:\")\n",
    "display(dbutils.fs.ls(f\"{batch_path}/mod2=1\")) \n",
    "\n",
    "print(\"\\nâ¡ï¸ Batch snapshot:\")\n",
    "display(spark.read.format(\"delta\").load(batch_path).orderBy(\"id\"))\n",
    "\n",
    "print(\"ğŸ”¶ Batch DESCRIBE HISTORY:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{batch_path}`\").show(truncate=False)\n",
    "\n",
    "# Count how many log files we have\n",
    "logs = dbutils.fs.ls(f\"{batch_path}/_delta_log\")\n",
    "print(f\"Batch commits (_delta_log count): {len([f for f in logs if f.name.endswith('.json')])}\")\n",
    "\n",
    "\n",
    "\n",
    "# 2) STREAMING WRITE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "stream_path     = \"dbfs:/FileStore/delta_demo_stream\"\n",
    "checkpoint_path = \"dbfs:/FileStore/delta_demo_stream_ckpt\"\n",
    "\n",
    "# dummy 5 rows/sec stream of increasing IDs\n",
    "stream_df = (spark.readStream\n",
    "                .format(\"rate\")\n",
    "                .option(\"rowsPerSecond\", 5)\n",
    "                .load()\n",
    "                .selectExpr(\"value AS id\"))\n",
    "\n",
    "query = (stream_df.writeStream\n",
    "                .format(\"delta\")\n",
    "                .option(\"checkpointLocation\", checkpoint_path)\n",
    "                .outputMode(\"append\")\n",
    "                # run once over all available data, then stop\n",
    "                .trigger(availableNow=True)\n",
    "                .start(stream_path))\n",
    "\n",
    "# let it fire off a couple of micro-batches\n",
    "query.awaitTermination(20000)  \n",
    "query.stop()\n",
    "print(\"ğŸ”· Stream DESCRIBE HISTORY:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{stream_path}`\").show(truncate=False)\n",
    "\n",
    "logs = dbutils.fs.ls(f\"{stream_path}/_delta_log\")\n",
    "print(f\"Stream commits (_delta_log count): {len([f for f in logs if f.name.endswith('.json')])}\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Streaming write â€” directory listing:\")\n",
    "display(dbutils.fs.ls(stream_path))\n",
    "\n",
    "print(\"ğŸ” _delta_log entries:\")\n",
    "display(dbutils.fs.ls(stream_path + \"/_delta_log\"))\n",
    "\n",
    "print(\"\\nâ¡ï¸ Stream snapshot:\")\n",
    "display(spark.read.format(\"delta\").load(stream_path).orderBy(\"id\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Unified_batch_ Stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
