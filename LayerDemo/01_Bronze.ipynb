{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94df16ad-74f0-4046-8136-1e0100db0a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#01_Bronze\n",
    "# Databricks notebook source\n",
    "#01_Bronze notebook\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import current_timestamp, lit, input_file_name\n",
    "import os\n",
    "import builtins\n",
    "\n",
    "# Delete the old Bronze folder entirely\n",
    "dbutils.fs.rm(\"dbfs:/tmp/bronze/cc_events/\", recurse=True)\n",
    "\n",
    "# 1) Derive a unique process_id\n",
    "all_conf   = spark.conf.getAll   # note the ()\n",
    "job_id     = all_conf.get(\"spark.databricks.job.id\",   \"interactive\")\n",
    "run_id     = all_conf.get(\"spark.databricks.job.runId\", \"interactive\")\n",
    "process_id = f\"daily_ingest|{job_id}_{run_id}\"\n",
    "\n",
    "\n",
    "# 1) derive process_id with safe fallback\n",
    "\n",
    "\n",
    "# 2) Define the Bronze schema (exactly what Silver expects)\n",
    "bronze_schema = StructType([\n",
    "    StructField(\"user_id\",         StringType(), True),\n",
    "    StructField(\"app_name\",        StringType(), True),\n",
    "    StructField(\"event_type\",      StringType(), True),\n",
    "    StructField(\"event_timestamp\", StringType(), True),\n",
    "    StructField(\"device\", StructType([\n",
    "        StructField(\"os\",     StringType(), True),\n",
    "        StructField(\"region\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# 3) Ensure the raw folder exists under DBFS /tmp\n",
    "raw_dbfs = \"dbfs:/tmp/raw/telemetry/\"\n",
    "raw_local = \"/dbfs/tmp/raw/telemetry/\"\n",
    "if not os.path.exists(raw_local):\n",
    "    dbutils.fs.mkdirs(raw_dbfs)\n",
    "    # Optionally burst in sample JSON here for a first run:\n",
    "    # dbutils.fs.put(f\"{raw_dbfs}sample1.json\", '{\"user_id\":\"u1\",\"app_name\":\"PS\",...}', True)\n",
    "\n",
    "# 4) Read the raw JSON into our Bronze schema\n",
    "raw_df = spark.read \\\n",
    "    .schema(bronze_schema) \\\n",
    "    .option(\"multiline\", True) \\\n",
    "    .json(raw_dbfs)\n",
    "\n",
    "# 5) Enrich with audit columns\n",
    "bronze_df = (raw_df\n",
    "    .withColumn(\"ingest_ts\",   current_timestamp())\n",
    "    .withColumn(\"process_id\",  lit(process_id))\n",
    ")\n",
    "\n",
    "# 6) Write to Bronze Delta (append-only)\n",
    "bronze_path = \"dbfs:/tmp/bronze/cc_events/\"\n",
    "bronze_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(bronze_path)\n",
    "\n",
    "# 7) Sanity-check your Bronze table\n",
    "display(spark.read.format(\"delta\").load(bronze_path))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Gold layer code \n",
    "# ───────────────────────────────────────────────────────────\n",
    "# Gold Layer: Curated, Consumption-Ready Fact Tables\n",
    "# ───────────────────────────────────────────────────────────\n",
    "\n",
    "from pyspark.sql.functions import count, countDistinct\n",
    "\n",
    "# 1) Define paths\n",
    "silver_path           = \"dbfs:/tmp/silver/cc_events_enterprise/\"\n",
    "feature_usage_gold    = \"dbfs:/tmp/gold/feature_usage_fact/\"\n",
    "user_activity_gold    = \"dbfs:/tmp/gold/user_activity_fact/\"\n",
    "\n",
    "# 2) Read the Silver “enterprise view”\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "display(silver_df.limit(5))\n",
    "\n",
    "# 3) Feature Usage Fact\n",
    "feature_usage = (\n",
    "    silver_df\n",
    "      .filter(\"feature_category IS NOT NULL\")                  # only track known features\n",
    "      .groupBy(\"event_date\", \"app_name\", \"feature_category\")   # natural grain\n",
    "      .agg(count(\"*\").alias(\"usage_count\"))                    # daily usage per feature\n",
    ")\n",
    "\n",
    "# 4) Write Feature Usage to Delta, partitioned by event_date\n",
    "feature_usage.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .save(feature_usage_gold)\n",
    "\n",
    "display(spark.read.format(\"delta\").load(feature_usage_gold))\n",
    "\n",
    "# 5) User Activity Fact\n",
    "user_activity = (\n",
    "    silver_df\n",
    "      .groupBy(\"event_date\", \"app_name\", \"region\")\n",
    "      .agg(countDistinct(\"user_id\").alias(\"active_users\"))     # daily unique users\n",
    ")\n",
    "\n",
    "# 6) Write User Activity to Delta, partitioned by event_date\n",
    "user_activity.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .save(user_activity_gold)\n",
    "\n",
    "display(spark.read.format(\"delta\").load(user_activity_gold))\n",
    "\n",
    "# 7) Optimize Gold tables for speed (Z-Order on high-cardinality column)\n",
    "spark.sql(f\"OPTIMIZE delta.`{feature_usage_gold}` ZORDER BY (feature_category)\")\n",
    "spark.sql(f\"OPTIMIZE delta.`{user_activity_gold}`   ZORDER BY (region)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
