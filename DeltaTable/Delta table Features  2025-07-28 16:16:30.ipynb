{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "474ccd2c-fa28-4b60-8610-bc366a696ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ACID Properties\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def list_parquet_files(directory):\n",
    "    print(f\"\\n=== Parquet files in: {directory} ===\")\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".parquet\"):\n",
    "                print(os.path.join(root, file))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "path  = \"/local_disk0/tmp/delta_acid_demo\"\n",
    "\n",
    "# ── Step A: VERSION 0 ──\n",
    "# Create a brand-new Delta table with 5 rows\n",
    "spark.range(0, 5) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .save(path)\n",
    "\n",
    "list_parquet_files(path)\n",
    "\n",
    "# ── Step B: VERSION 1 ──\n",
    "# Append 5 more rows\n",
    "spark.range(5, 10) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"append\") \\\n",
    "     .save(path)\n",
    "\n",
    "list_parquet_files(path)\n",
    "\n",
    "# Step C: VERSION 2 ──\n",
    "deltaTable = DeltaTable.forPath(spark, path)\n",
    "deltaTable.update(\n",
    "     condition = \"id == 2\",\n",
    "     set = {\"id\": \"200\"}\n",
    ")\n",
    "list_parquet_files(path)\n",
    "\n",
    "print (\"History\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{path}`\").show(truncate=False)\n",
    "\n",
    "print(\"-version 0\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\",0).load(path).show()\n",
    "\n",
    "print(\"-version 1\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\",1).load(path).show()  \n",
    "\n",
    "print(\" Latest version\")\n",
    "spark.read.format(\"delta\").load(path).show()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc2fd7b-8d72-44a1-bb8b-06ebf7fb425b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 1: SETUP\n",
    "# Paths for the Delta table and the streaming checkpoint\n",
    "delta_path = \"dbfs:/FileStore/delta_unified_demo\"\n",
    "checkpoint = \"dbfs:/FileStore/delta_unified_demo_checkpoint\"\n",
    "\n",
    "# Clean up old data if present\n",
    "dbutils.fs.rm(delta_path, recurse=True)\n",
    "dbutils.fs.rm(checkpoint, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bd9faabf-18ec-4e64-8612-8501ceb6b95e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# aspect 3 - Unified for demo unified batch and Stream \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# CLEAN UP\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/delta_demo_batch\", recurse=True)\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/delta_demo_stream\", recurse=True)\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/delta_demo_stream_ckpt\", recurse=True)\n",
    "\n",
    "# 1) BATCH WRITE\n",
    "# ────────────────────────────────────────────────────────────────────────\n",
    "# write IDs 0–9 in a single batch, *partitioned* on mod2 so you see two folders\n",
    "batch_path = \"dbfs:/FileStore/delta_demo_batch\"\n",
    "\n",
    "spark.range(0, 10) \\\n",
    "     .withColumn(\"mod2\", col(\"id\") % 2) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .partitionBy(\"mod2\") \\\n",
    "     .save(batch_path)\n",
    "\n",
    "print(\"📦 Batch write — directory listing:\")\n",
    "display(dbutils.fs.ls(batch_path))           \n",
    "\n",
    "print(\"🔎 Inside mod2=0 folder:\")\n",
    "display(dbutils.fs.ls(f\"{batch_path}/mod2=0\"))  \n",
    "print(\"🔎 Inside mod2=1 folder:\")\n",
    "display(dbutils.fs.ls(f\"{batch_path}/mod2=1\")) \n",
    "\n",
    "print(\"\\n➡️ Batch snapshot:\")\n",
    "display(spark.read.format(\"delta\").load(batch_path).orderBy(\"id\"))\n",
    "\n",
    "print(\"🔶 Batch DESCRIBE HISTORY:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{batch_path}`\").show(truncate=False)\n",
    "\n",
    "# Count how many log files we have\n",
    "logs = dbutils.fs.ls(f\"{batch_path}/_delta_log\")\n",
    "print(f\"Batch commits (_delta_log count): {len([f for f in logs if f.name.endswith('.json')])}\")\n",
    "\n",
    "\n",
    "\n",
    "# 2) STREAMING WRITE\n",
    "# ────────────────────────────────────────────────────────────────────────\n",
    "stream_path     = \"dbfs:/FileStore/delta_demo_stream\"\n",
    "checkpoint_path = \"dbfs:/FileStore/delta_demo_stream_ckpt\"\n",
    "\n",
    "# dummy 5 rows/sec stream of increasing IDs\n",
    "stream_df = (spark.readStream\n",
    "                .format(\"rate\")\n",
    "                .option(\"rowsPerSecond\", 5)\n",
    "                .load()\n",
    "                .selectExpr(\"value AS id\"))\n",
    "\n",
    "query = (stream_df.writeStream\n",
    "                .format(\"delta\")\n",
    "                .option(\"checkpointLocation\", checkpoint_path)\n",
    "                .outputMode(\"append\")\n",
    "                # run once over all available data, then stop\n",
    "                .trigger(availableNow=True)\n",
    "                .start(stream_path))\n",
    "\n",
    "# let it fire off a couple of micro-batches\n",
    "query.awaitTermination(20000)  \n",
    "query.stop()\n",
    "print(\"🔷 Stream DESCRIBE HISTORY:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{stream_path}`\").show(truncate=False)\n",
    "\n",
    "logs = dbutils.fs.ls(f\"{stream_path}/_delta_log\")\n",
    "print(f\"Stream commits (_delta_log count): {len([f for f in logs if f.name.endswith('.json')])}\")\n",
    "\n",
    "print(\"\\n📦 Streaming write — directory listing:\")\n",
    "display(dbutils.fs.ls(stream_path))\n",
    "\n",
    "print(\"🔎 _delta_log entries:\")\n",
    "display(dbutils.fs.ls(stream_path + \"/_delta_log\"))\n",
    "\n",
    "print(\"\\n➡️ Stream snapshot:\")\n",
    "display(spark.read.format(\"delta\").load(stream_path).orderBy(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "152dd5bc-53a2-4e59-965c-bc878f759a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aspect 4 - Schema Evolution \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "path = \"dbfs:/FileStore/delta_schema_demo\"\n",
    "\n",
    "# CLEAN UP\n",
    "dbutils.fs.rm(path, recurse=True)\n",
    "\n",
    "# ── Step A: INITIAL WRITE ──\n",
    "# Create a table with schema (id: LONG, val: STRING)\n",
    "spark.range(0, 3) \\\n",
    "     .withColumn(\"val\", lit(\"alpha\")) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .save(path)\n",
    "\n",
    "print(\"Version 0 snapshot:\")\n",
    "display(spark.read.format(\"delta\").load(path))\n",
    "\n",
    "# ── Step B: TRY A MISMATCHED WRITE ──\n",
    "# This DataFrame has an extra column 'new_col'\n",
    "bad_df = spark.range(3, 6) \\\n",
    "              .withColumn(\"val\", lit(\"beta\")) \\\n",
    "              .withColumn(\"new_col\", lit(99.9))\n",
    "\n",
    "try:\n",
    "    bad_df.write.format(\"delta\").mode(\"append\").save(path)\n",
    "except Exception as e:\n",
    "    print(\"🛑 Write rejected due to schema mismatch:\\n\", e)\n",
    "\n",
    "# ── Step C: ALLOW EVOLUTION & APPEND ──\n",
    "# Now enable schema merging to accept the new column\n",
    "bad_df.write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"append\") \\\n",
    "     .option(\"mergeSchema\", \"true\") \\\n",
    "     .save(path)\n",
    "\n",
    "print(\"Version 2 snapshot (merged schema):\")\n",
    "spark.read \\\n",
    "     .format(\"delta\") \\\n",
    "     .option(\"mergeSchema\", \"true\") \\\n",
    "     .load(path) \\\n",
    "     .show()\n",
    "\n",
    "# ── Inspect history & schema ──\n",
    "print(\"DESCRIBE HISTORY:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{path}`\").show(truncate=False)\n",
    "\n",
    "print(\"Current schema:\")\n",
    "spark.read.format(\"delta\").load(path).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58843759-c49e-48af-8322-a4952f4499af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ASpect 5 - Time travel and versioning\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "path = \"/local_disk0/tmp/delta_time_travel\"\n",
    "\n",
    "# CLEAN UP\n",
    "dbutils.fs.rm(path, recurse=True)\n",
    "\n",
    "# ── Version 0: Initial write ──\n",
    "spark.range(0, 3) \\\n",
    "     .withColumn(\"val\", lit(\"alpha\")) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .save(path)\n",
    "\n",
    "# ── Version 1: Append new rows ──\n",
    "spark.range(3, 5) \\\n",
    "     .withColumn(\"val\", lit(\"beta\")) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"append\") \\\n",
    "     .save(path)\n",
    "\n",
    "# ── Version 2: Update in place ──\n",
    "DeltaTable.forPath(spark, path) \\\n",
    "  .update(\n",
    "    condition = \"id == 1\",\n",
    "    set       = {\"id\": \"100\", \"val\": \"'gamma'\"}\n",
    "  )\n",
    "\n",
    "# 1) Show commit history\n",
    "print(\"=== DESCRIBE HISTORY ===\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{path}`\").show(truncate=False)\n",
    "\n",
    "# 2) Read as of version 0\n",
    "print(\"→ VERSION 0 SNAPSHOT:\")\n",
    "spark.read.format(\"delta\") \\\n",
    "     .option(\"versionAsOf\", 0) \\\n",
    "     .load(path) \\\n",
    "     .orderBy(\"id\") \\\n",
    "     .show()\n",
    "\n",
    "# 3) Read as of version 1\n",
    "print(\"→ VERSION 1 SNAPSHOT:\")\n",
    "spark.read.format(\"delta\") \\\n",
    "     .option(\"versionAsOf\", 1) \\\n",
    "     .load(path) \\\n",
    "     .orderBy(\"id\") \\\n",
    "     .show()\n",
    "\n",
    "# 4) Read latest (version 2)\n",
    "print(\"→ VERSION 2 (LATEST):\")\n",
    "spark.read.format(\"delta\") \\\n",
    "     .load(path) \\\n",
    "     .orderBy(\"id\") \\\n",
    "     .show()\n",
    "\n",
    "# 5) Read by timestamp (pick a timestamp between v1 & v2)\n",
    "import datetime\n",
    "ts = spark.sql(f\"SELECT timestamp FROM (DESCRIBE HISTORY delta.`{path}`) WHERE version = 1\").first()[0]\n",
    "print(f\"→ SNAPSHOT AS OF {ts}:\")\n",
    "spark.read.format(\"delta\") \\\n",
    "     .option(\"timestampAsOf\", ts) \\\n",
    "     .load(path) \\\n",
    "     .orderBy(\"id\") \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2b2a514b-2cb0-4b52-ae1d-19827bc9a2db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ASpect 5 - Merge feature\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "path = \"dbfs:/FileStore/delta_merge_demo\"\n",
    "\n",
    "# CLEAN UP\n",
    "dbutils.fs.rm(path, recurse=True)\n",
    "\n",
    "# ── Step A: Create base table ──\n",
    "# IDs 1–4 with vals A–D\n",
    "spark.createDataFrame(\n",
    "    [(1, \"A\"), (2, \"B\"), (3, \"C\"), (4, \"D\")],\n",
    "    [\"id\", \"val\"]\n",
    ").write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "\n",
    "print(\"Initial (version 0):\")\n",
    "display(spark.read.format(\"delta\").load(path).orderBy(\"id\"))\n",
    "\n",
    "# ── Step B: Prepare updates ──\n",
    "# id=1 updated, id=2 marked for delete, id=5 new\n",
    "updates = spark.createDataFrame(\n",
    "    [(1, \"A'\"), (2, None), (5, \"E\")],\n",
    "    [\"id\", \"val\"]\n",
    ")\n",
    "\n",
    "# ── Step C: MERGE\n",
    "delta_tbl = DeltaTable.forPath(spark, path)\n",
    "(delta_tbl.alias(\"t\")\n",
    "  .merge(\n",
    "     source = updates.alias(\"s\"),\n",
    "     condition = \"t.id = s.id\"\n",
    "  )\n",
    "  .whenMatchedUpdate(condition=\"s.val IS NOT NULL\", set={\"val\": \"s.val\"})\n",
    "  .whenMatchedDelete(condition=\"s.val IS NULL\")\n",
    "  .whenNotMatchedInsert(values={\"id\": \"s.id\", \"val\": \"s.val\"})\n",
    "  .execute()\n",
    ")\n",
    "\n",
    "print(\"After MERGE (version 1):\")\n",
    "display(spark.read.format(\"delta\").load(path).orderBy(\"id\"))\n",
    "\n",
    "# ── Inspect history ──\n",
    "print(\"DESCRIBE HISTORY:\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{path}`\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c856307e-0c98-4e34-9036-bfebdc282756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ASpect 6 - Zorder \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 1) Build a “wide” table: 1M rows, random country & age\n",
    "import random\n",
    "data = [(i,\n",
    "         random.choice([\"US\",\"CA\",\"MX\",\"IN\",\"DE\"]),\n",
    "         random.randint(1,100))\n",
    "        for i in range(1_000_000)]\n",
    "df = spark.createDataFrame(data, schema=[\"id\",\"country\",\"age\"])\n",
    "\n",
    "# 2) Write it as Delta (no partitioning)\n",
    "path = \"/tmp/delta_skip_zorder\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "\n",
    "# Read before Z-order\n",
    "# simple filter on age\n",
    "filtered = spark.read.format(\"delta\") \\\n",
    "    .load(path) \\\n",
    "    .filter(\"age BETWEEN 30 AND 40\")\n",
    "\n",
    "# show that many files are scanned\n",
    "print(\"Files scanned before ZORDER:\")\n",
    "filtered.explain(True)\n",
    "\n",
    "# Now cluster\n",
    "spark.sql(f\"OPTIMIZE delta.`{path}` ZORDER BY age\")\n",
    "\n",
    "# Read after Z-order\n",
    "filtered2 = spark.read.format(\"delta\") \\\n",
    "    .load(path) \\\n",
    "    .filter(\"age BETWEEN 30 AND 40\")\n",
    "print(\"Files scanned after ZORDER:\")\n",
    "filtered2.explain(True)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "filtered.count()\n",
    "print(\"Before:\", time.time() - t0, \"s\")\n",
    "\n",
    "t1 = time.time()\n",
    "filtered2.count()\n",
    "print(\"After ZORDER:\", time.time() - t1, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8061160-727d-4588-8425-cf77a332fb74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ASpect 8 Audit and Lineage \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Paths for our demo tables\n",
    "src = \"/tmp/delta_audit_src\"\n",
    "dst = \"/tmp/delta_audit_child\"\n",
    "\n",
    "# 1) Clean up any previous runs\n",
    "dbutils.fs.rm(src, recurse=True)\n",
    "dbutils.fs.rm(dst, recurse=True)\n",
    "\n",
    "# 2) Create source table (version 0)\n",
    "spark.range(0, 3) \\\n",
    "     .withColumn(\"val\", lit(\"alpha\")) \\\n",
    "     .write.format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .save(src)\n",
    "\n",
    "# 3) Append more data (version 1)\n",
    "spark.range(3, 5) \\\n",
    "     .withColumn(\"val\", lit(\"beta\")) \\\n",
    "     .write.format(\"delta\") \\\n",
    "     .mode(\"append\") \\\n",
    "     .save(src)\n",
    "\n",
    "# 4) Create a child table via CTAS (child version 0)\n",
    "spark.sql(f\"CREATE TABLE delta.`{dst}` AS SELECT * FROM delta.`{src}`\")\n",
    "\n",
    "# 5) Inspect Audit History on source_tbl\n",
    "print(\"=== SOURCE TABLE HISTORY ===\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{src}`\").show(truncate=False)\n",
    "\n",
    "# 6) Inspect Audit History (and lineage) on child_tbl\n",
    "print(\"=== CHILD TABLE HISTORY ===\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{dst}`\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f0d065e-e2f6-4a84-be59-b5bc67fb7ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CELL 2: BATCH WRITE\n",
    "# Write IDs 0–4 in one batch\n",
    "spark.range(0, 5) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .save(delta_path)\n",
    "\n",
    "print(\"Batch data:\")\n",
    "display(spark.read.format(\"delta\").load(delta_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "71e2aedc-1a50-497f-a6e2-569449ed8498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Can skip - since there is one more code sample one below \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "delta_path = \"dbfs:/FileStore/delta_stream_demo\"\n",
    "checkpoint =  \"dbfs:/FileStore/delta_stream_demo_ckpt\"\n",
    "\n",
    "# 1) start a streaming write: \n",
    "stream_df = (\n",
    "    spark.readStream.format(\"rate\")\n",
    "         .option(\"rowsPerSecond\", 10)   # 10 events/sec\n",
    "         .load()\n",
    "         .selectExpr(\"value AS event_id\")\n",
    ")\n",
    "\n",
    "stream_query = (\n",
    "    stream_df\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", checkpoint)\n",
    "      .outputMode(\"append\")\n",
    "      .trigger(availableNow=True)\n",
    "      .start(delta_path)\n",
    ")\n",
    "\n",
    "# Let it run for a few seconds, then stop\n",
    "import time; time.sleep(5)\n",
    "stream_query.stop()\n",
    "\n",
    "# 2) Inspect the files & log\n",
    "print(\"Files on disk:\")\n",
    "display(dbutils.fs.ls(delta_path))\n",
    "print(\"\\nDelta log entries:\")\n",
    "display(dbutils.fs.ls(delta_path + \"/_delta_log\"))\n",
    "print(\"\\nLatest snapshot:\")\n",
    "display(spark.read.format(\"delta\").load(delta_path).orderBy(\"event_id\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta table Features  2025-07-28 16:16:30",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
