{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7f31c697-4571-4c9b-a543-451c3526bd36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#09 CDF example \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Silver_Incr_DIY\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "\n",
    "BRONZE_TABLE = \"demo.bronze_events\"\n",
    "SILVER_TABLE = \"demo.silver_events\"\n",
    "CHECKPOINT_TABLE = \"demo.silver_checkpoint\"\n",
    "\n",
    "def process_silver_diy():\n",
    "    # 1) Read last watermark\n",
    "    ckpt = spark.table(CHECKPOINT_TABLE).select(\"last_ts\").collect()\n",
    "    last_ts = ckpt[0][\"last_ts\"] if ckpt else \"1970-01-01\"\n",
    "\n",
    "    # 2) Pull only new rows from Bronze by ingest_ts\n",
    "    bronze_df = spark.table(BRONZE_TABLE)\n",
    "    new_df = bronze_df.filter(col(\"ingest_ts\") > last_ts)\n",
    "\n",
    "    # 3) Clean / transform\n",
    "    clean_df = (new_df\n",
    "        .filter(col(\"ts\").isNotNull())\n",
    "        .withColumn(\"ts\", col(\"ts\").cast(\"timestamp\")))\n",
    "\n",
    "    # 4) Upsert into Silver via MERGE\n",
    "    silver = DeltaTable.forName(spark, SILVER_TABLE)\n",
    "    (silver.alias(\"t\")\n",
    "          .merge(clean_df.alias(\"s\"), \"t.event_id = s.event_id\")\n",
    "          .whenMatchedUpdateAll()\n",
    "          .whenNotMatchedInsertAll()\n",
    "          .execute())\n",
    "\n",
    "    # 5) Update watermark to max ingest_ts processed\n",
    "    max_ts = clean_df.agg({\"ingest_ts\": \"max\"}).collect()[0][0]\n",
    "    spark.createDataFrame([(max_ts,)], [\"last_ts\"]) \\\n",
    "         .write.format(\"delta\") \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .saveAsTable(CHECKPOINT_TABLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b38610dc-1102-4d7e-a6ea-9fe2a3eb1914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 1) Init Spark with Hive support\n",
    "# ——————————————————————————————————————————————\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"Enable_CDF_Bronze_HMS\")\n",
    "      .enableHiveSupport()            # so spark_catalog points at your HMS\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 2) Point at the Hive Metastore (not UC)\n",
    "# ——————————————————————————————————————————————\n",
    "spark.sql(\"USE CATALOG spark_catalog\")\n",
    "\n",
    "# (Optional) switch to your database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo\")\n",
    "spark.sql(\"USE demo\")\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 3) (Re)create Bronze table in HMS if not already registered\n",
    "# ——————————————————————————————————————————————\n",
    "bronze_path = \"dbfs:/mnt/data/bronze_events\"  # or s3://… / abfss://… etc.\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS bronze_events (\n",
    "    event_id     STRING,\n",
    "    user_id      STRING,\n",
    "    event_type   STRING,\n",
    "    ts           STRING,\n",
    "    ingest_ts    TIMESTAMP\n",
    "  )\n",
    "  USING DELTA\n",
    "  LOCATION '{bronze_path}'\n",
    "\"\"\")\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 4) Enable Change Data Feed on Bronze\n",
    "# ——————————————————————————————————————————————\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE bronze_events\n",
    "  SET TBLPROPERTIES (\n",
    "    delta.enableChangeDataFeed = true\n",
    "  )\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Change Data Feed has been ENABLED on demo.bronze_events in the Hive Metastore.\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Silver_Incr_CDF\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate())\n",
    "\n",
    "BRONZE_TABLE = \"demo.bronze_events\"\n",
    "SILVER_TABLE = \"demo.silver_events\"\n",
    "CDF_CHECKPOINT = \"demo.silver_cdf_checkpoint\"\n",
    "\n",
    "def process_silver_cdf():\n",
    "    # 1) Read last processed version\n",
    "    ckpt = spark.table(CDF_CHECKPOINT).select(\"version\").collect()\n",
    "    last_version = ckpt[0][\"version\"] if ckpt else 0\n",
    "\n",
    "    # 2) Get current Bronze version\n",
    "    current_version = spark.sql(f\"DESCRIBE HISTORY {BRONZE_TABLE}\") \\\n",
    "                           .selectExpr(\"max(version) as v\").collect()[0][\"v\"]\n",
    "\n",
    "    # 3) Read *all* inserts/updates/deletes via CDF\n",
    "    cdf_df = (spark.read.format(\"delta\")\n",
    "                   .option(\"readChangeData\", \"true\")\n",
    "                   .option(\"startingVersion\", last_version)\n",
    "                   .option(\"endingVersion\", current_version)\n",
    "                   .table(BRONZE_TABLE))\n",
    "\n",
    "    # 4) Clean / filter only actual rows (isChange == 'update_postimage' or 'insert')\n",
    "    upserts = (cdf_df\n",
    "        .filter(col(\"_change_type\") != \"delete\")\n",
    "        .withColumn(\"ts\", col(\"ts\").cast(\"timestamp\")))\n",
    "\n",
    "    # 5) Upsert into Silver\n",
    "    silver = DeltaTable.forName(spark, SILVER_TABLE)\n",
    "    (silver.alias(\"t\")\n",
    "           .merge(upserts.alias(\"s\"), \"t.event_id = s.event_id\")\n",
    "           .whenMatchedUpdateAll()\n",
    "           .whenNotMatchedInsertAll()\n",
    "           .execute())\n",
    "\n",
    "    # 6) Update CDF checkpoint to current_version\n",
    "    spark.createDataFrame([(current_version,)], [\"version\"]) \\\n",
    "         .write.format(\"delta\") \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .saveAsTable(CDF_CHECKPOINT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7923545c-2927-4365-9620-4341132bac5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 0) Bootstrap Spark with Hive support\n",
    "# --------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"Silver_Incr_Driver\")\n",
    "      .enableHiveSupport()\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1) Point at Hive Metastore & demo database\n",
    "# --------------------------------------------------------------------\n",
    "spark.sql(\"USE CATALOG spark_catalog\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo\")\n",
    "spark.sql(\"USE demo\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2) (Re)create Bronze table & enable CDF\n",
    "# --------------------------------------------------------------------\n",
    "bronze_path = \"dbfs:/mnt/data/bronze_events\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS bronze_events (\n",
    "    event_id   STRING,\n",
    "    user_id    STRING,\n",
    "    event_type STRING,\n",
    "    ts         STRING,\n",
    "    ingest_ts  TIMESTAMP\n",
    "  )\n",
    "  USING DELTA\n",
    "  LOCATION '{bronze_path}'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  ALTER TABLE bronze_events\n",
    "  SET TBLPROPERTIES (\n",
    "    delta.enableChangeDataFeed = true\n",
    "  )\n",
    "\"\"\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3) DIY Silver processor\n",
    "# --------------------------------------------------------------------\n",
    "CHECKPOINT_TABLE = \"demo.silver_checkpoint\"\n",
    "SILVER_TABLE    = \"demo.silver_events\"\n",
    "BRONZE_TABLE    = \"demo.bronze_events\"\n",
    "\n",
    "def process_silver_diy():\n",
    "    # 3.1 Read last watermark\n",
    "    ckpt = spark.table(CHECKPOINT_TABLE).select(\"last_ts\").collect()\n",
    "    last_ts = ckpt[0][\"last_ts\"] if ckpt else \"1970-01-01\"\n",
    "\n",
    "    # 3.2 Pull only new rows via ingest_ts\n",
    "    new_df = (spark.table(BRONZE_TABLE)\n",
    "                  .filter(col(\"ingest_ts\") > last_ts))\n",
    "\n",
    "    # 3.3 Clean / transform\n",
    "    clean_df = (new_df\n",
    "        .filter(col(\"ts\").isNotNull())\n",
    "        .withColumn(\"ts\", col(\"ts\").cast(\"timestamp\")))\n",
    "\n",
    "    # 3.4 Upsert into Silver\n",
    "    silver = DeltaTable.forName(spark, SILVER_TABLE)\n",
    "    (silver.alias(\"t\")\n",
    "          .merge(clean_df.alias(\"s\"), \"t.event_id = s.event_id\")\n",
    "          .whenMatchedUpdateAll()\n",
    "          .whenNotMatchedInsertAll()\n",
    "          .execute())\n",
    "\n",
    "    # 3.5 Update watermark\n",
    "    max_ts = clean_df.agg({\"ingest_ts\": \"max\"}).collect()[0][0]\n",
    "    spark.createDataFrame([(max_ts,)], [\"last_ts\"]) \\\n",
    "         .write.format(\"delta\") \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .saveAsTable(CHECKPOINT_TABLE)\n",
    "\n",
    "    print(\"✅ DIY Silver update complete. Watermark =\", max_ts)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4) CDF-based Silver processor\n",
    "# --------------------------------------------------------------------\n",
    "CDF_CHECKPOINT = \"demo.silver_cdf_checkpoint\"\n",
    "\n",
    "def process_silver_cdf():\n",
    "    # 4.1 Read last processed version\n",
    "    ckpt = spark.table(CDF_CHECKPOINT).select(\"version\").collect()\n",
    "    last_version = ckpt[0][\"version\"] if ckpt else 0\n",
    "\n",
    "    # 4.2 Discover current Bronze version\n",
    "    current_version = (spark\n",
    "        .sql(f\"DESCRIBE HISTORY {BRONZE_TABLE}\")\n",
    "        .selectExpr(\"max(version) as v\")\n",
    "        .collect()[0][\"v\"])\n",
    "\n",
    "    # 4.3 Read CDF\n",
    "    cdf_df = (spark.read.format(\"delta\")\n",
    "                   .option(\"readChangeData\", \"true\")\n",
    "                   .option(\"startingVersion\", last_version)\n",
    "                   .option(\"endingVersion\",   current_version)\n",
    "                   .table(BRONZE_TABLE))\n",
    "\n",
    "    # 4.4 Filter for inserts & update_postimage\n",
    "    upserts = (cdf_df\n",
    "        .filter(col(\"_change_type\") != \"delete\")\n",
    "        .withColum(\"ts\", col(\"ts\").cast(\"timestamp\")))\n",
    "\n",
    "    # 4.5 Merge into Silver\n",
    "    silver = DeltaTable.forName(spark, SILVER_TABLE)\n",
    "    (silver.alias(\"t\")\n",
    "           .merge(upserts.alias(\"s\"), \"t.event_id = s.event_id\")\n",
    "           .whenMatchedUpdateAll()\n",
    "           .whenNotMatchedInsertAll()\n",
    "           .execute())\n",
    "\n",
    "    # 4.6 Advance version checkpoint\n",
    "    spark.createDataFrame([(current_version,)], [\"version\"]) \\\n",
    "         .write.format(\"delta\") \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .saveAsTable(CDF_CHECKPOINT)\n",
    "\n",
    "    print(f\"✅ CDF Silver update complete. Processed versions {last_version+1}→{current_version}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5) Driver logic: choose which processor to run\n",
    "# --------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    mode = spark.conf.get(\"spark.silver.mode\", \"DIY\")  # or pass via --conf silver.mode=CDF\n",
    "\n",
    "    if mode.upper() == \"DIY\":\n",
    "        process_silver_diy()\n",
    "    elif mode.upper() == \"CDF\":\n",
    "        process_silver_cdf()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown silver.mode: {mode}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09_CDF_sample",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
