{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ca6a4e-2896-4a0d-9be3-ea608db6999b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Silver Layer Ingestion for telemetry1 events\n",
    "# Spark 4.0.0, serverless cluster compatible\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    current_timestamp, lit, col, date_format,\n",
    "    lower, trim, monotonically_increasing_id,\n",
    "    when, substring, concat, date_sub, current_date\n",
    ")\n",
    "from pyspark.sql.types import TimestampType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 0) Initialize an empty user_profile Delta table so the subsequent read wonâ€™t fail\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "# Paths for Bronze, Silver, Quarantine, and User Profile dimensions\n",
    "bronze_path       = \"dbfs:/tmp/bronze/telemetry1/\"\n",
    "silver_path       = \"dbfs:/tmp/silver/telemetry1/\"\n",
    "quarantine_path   = \"dbfs:/tmp/quarantine/telemetry1/\"\n",
    "user_profile_path = \"dbfs:/tmp/dim/user_profile/\"\n",
    "\n",
    "\n",
    "user_profile_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Write an empty DataFrame with that schema as a Delta table\n",
    "spark.createDataFrame([], user_profile_schema) \\\n",
    "    .write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(user_profile_path)\n",
    "\n",
    "\n",
    "# 0) Create the Silver folder (and any others you need)\n",
    "dbutils.fs.mkdirs(silver_path)\n",
    "dbutils.fs.mkdirs(quarantine_path)\n",
    "dbutils.fs.mkdirs(user_profile_path)\n",
    "\n",
    "# 0) Read Bronze Delta with schema merge to handle schema drift\n",
    "bronze_df = (\n",
    "    spark.read\n",
    "         .format(\"delta\")\n",
    "         .option(\"mergeSchema\", \"true\")\n",
    "         .load(bronze_path)\n",
    ")\n",
    "\n",
    "# 1) Null / Garbage-Value Removal\n",
    "clean_df = bronze_df.filter(\n",
    "    col(\"user_id\").isNotNull() &\n",
    "    col(\"app_name\").isNotNull() &\n",
    "    col(\"event_type\").isNotNull() &\n",
    "    (col(\"event_type\") != \"BAD_EVENT\") &\n",
    "    col(\"event_timestamp\").isNotNull()\n",
    ")\n",
    "\n",
    "# 2) Data-Quality Checks & Quarantines (event_timestamp not in future)\n",
    "now_ts = current_timestamp()\n",
    "good_df = clean_df.filter(col(\"event_timestamp\") <= now_ts)\n",
    "bad_df  = clean_df.filter(col(\"event_timestamp\") > now_ts)\n",
    "bad_df.write.mode(\"append\").json(quarantine_path)\n",
    "\n",
    "# Flatten nested device struct for dedupe and joins\n",
    "flat_df = good_df.withColumn(\"os\", col(\"device.os\")).withColumn(\"region\", col(\"device.region\"))\n",
    "\n",
    "# 3) Deduplication\n",
    "# Drop duplicates on user/app/event/time/os/region\n",
    "dedup_df = flat_df.dropDuplicates([\n",
    "    \"user_id\", \"app_name\", \"event_type\",\n",
    "    \"event_timestamp\", \"os\", \"region\"\n",
    "])\n",
    "\n",
    "# 4) Conformance Join to Dimensions\n",
    "def make_dim(data, cols): return spark.createDataFrame(data, cols)\n",
    "app_dim    = make_dim([(\"Photos\",1),(\"Mail\",2),(\"Chat\",3),(\"Store\",4),(\"Search\",5)], [\"app_name\",\"app_id\"])\n",
    "event_dim  = make_dim([(\"click\",1),(\"view\",2),(\"purchase\",3),(\"login\",4),(\"logout\",5),(\"scroll\",6)], [\"event_type\",\"event_type_id\"])\n",
    "os_dim      = make_dim([(\"iOS\",1),(\"Android\",2),(\"Windows\",3),(\"Linux\",4)], [\"os\",\"os_id\"])\n",
    "region_dim = make_dim([(\"IN\",1),(\"US\",2),(\"EU\",3),(\"APAC\",4),(\"LATAM\",5)], [\"region\",\"region_id\"])\n",
    "\n",
    "conf_df = (\n",
    "    dedup_df\n",
    "      .join(app_dim,    \"app_name\",  \"left\")\n",
    "      .join(event_dim,  \"event_type\",\"left\")\n",
    "      .join(os_dim,     \"os\",        \"left\")\n",
    "      .join(region_dim, \"region\",    \"left\")\n",
    ")\n",
    "\n",
    "# 5) Type Enforcement & Casting\n",
    "typed_df = (\n",
    "    conf_df\n",
    "      .withColumn(\"app_id\",         col(\"app_id\").cast(\"int\"))\n",
    "      .withColumn(\"event_type_id\",  col(\"event_type_id\").cast(\"int\"))\n",
    "      .withColumn(\"os_id\",          col(\"os_id\").cast(\"int\"))\n",
    "      .withColumn(\"region_id\",      col(\"region_id\").cast(\"int\"))\n",
    "      .withColumn(\"event_timestamp\",col(\"event_timestamp\").cast(TimestampType()))\n",
    ")\n",
    "\n",
    "# 6) Standardization & Normalization\n",
    "normalized_df = (\n",
    "    typed_df\n",
    "      .withColumn(\"app_name_norm\",   lower(trim(col(\"app_name\"))))\n",
    "      .withColumn(\"event_type_norm\", lower(trim(col(\"event_type\"))))\n",
    "      .withColumn(\"user_id_norm\",    trim(col(\"user_id\")))\n",
    ")\n",
    "\n",
    "# 7) Surrogate-Key Generation\n",
    "sk_df = normalized_df.withColumn(\"telemetry_sk\", monotonically_increasing_id())\n",
    "\n",
    "# 8) Metadata Enrichment (Silver timestamp)\n",
    "enriched_df = sk_df.withColumn(\"silver_ingest_ts\", current_timestamp())\n",
    "\n",
    "# 9) Business-Rule Derivations\n",
    "br_df = enriched_df.withColumn(\n",
    "    \"is_purchase\",\n",
    "    when(col(\"event_type_norm\") == \"purchase\", lit(1)).otherwise(lit(0))\n",
    ")\n",
    "\n",
    "# 10) Late-Arriving Data Management / Watermarking (keep last 7 days)\n",
    "recent_df = br_df.filter(\n",
    "    col(\"event_timestamp\") >= date_sub(current_date(), 7)\n",
    ")\n",
    "\n",
    "# 11) Row-Level Security / Masking of PII\n",
    "masked_df = recent_df.withColumn(\n",
    "    \"user_id_masked\",\n",
    "    concat(substring(col(\"user_id\"), 1, 3), lit(\"***\"))\n",
    ")\n",
    "\n",
    "# 12) Cross-Source Consolidation (e.g., user profile enrichment)\n",
    "user_profile_df  = spark.read.format(\"delta\").load(user_profile_path)\n",
    "consolidated_df  = masked_df.join(user_profile_df, \"user_id\", \"left\")\n",
    "\n",
    "# 13) SCD Prep (Effective dates & versioning)\n",
    "scd_df = (\n",
    "    consolidated_df\n",
    "      .withColumn(\"effective_from\", col(\"event_timestamp\"))\n",
    "      .withColumn(\"effective_to\",   lit(None).cast(TimestampType()))\n",
    "      .withColumn(\"version\",        lit(1))\n",
    ")\n",
    "\n",
    "# 14) Partitioning & File-Layout Optimization\n",
    "scd_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"app_id\") \\\n",
    "    .save(silver_path)\n",
    "\n",
    "# Optimize Silver layout\n",
    "silver_table = DeltaTable.forPath(spark, silver_path)\n",
    "silver_table.optimize().where(\"app_id IS NOT NULL\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Telemetry1_Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
