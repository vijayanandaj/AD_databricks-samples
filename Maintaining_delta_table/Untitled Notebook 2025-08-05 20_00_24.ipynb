{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b42ee3d-dece-489b-af02-4c8b2181fa47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) BRONZE ‚Äì land ‚Äúas-is‚Äù JSON into a Delta table, append-only\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "import os\n",
    "\n",
    "# -- 1a) prepare paths\n",
    "raw_path    = \"/tmp/raw/doc_events/\"\n",
    "bronze_path = \"/tmp/bronze/doc_events/\"\n",
    "\n",
    "all_conf   = spark.conf.getAll   # note the ()\n",
    "job_id     = all_conf.get(\"spark.databricks.job.id\",   \"interactive\")\n",
    "run_id     = all_conf.get(\"spark.databricks.job.runId\", \"interactive\")\n",
    "process_id = f\"daily_ingest|{job_id}_{run_id}\"\n",
    "\n",
    "# -- 1b) ensure the raw JSON exists (for demo we'll create a tiny sample if missing)\n",
    "if not os.path.exists(\"/dbfs\" + raw_path):\n",
    "    dbutils.fs.mkdirs(raw_path)\n",
    "    sample = \"\"\"\n",
    "    {\"event_id\":\"e1\",\"user_id\":\"u1\",\"doc_id\":\"d1\",\n",
    "     \"action\":\"view\",\"event_time\":\"2025-08-01T12:00:00Z\",\n",
    "     \"device\":{\"os\":\"iOS\",\"region\":\"CA\"}}\n",
    "    {\"event_id\":\"e2\",\"user_id\":\"u2\",\"doc_id\":\"d2\",\n",
    "     \"action\":\"edit\",\"event_time\":\"2025-08-01T12:05:00Z\",\n",
    "     \"device\":{\"os\":\"Android\",\"region\":\"NY\"}}\n",
    "    \"\"\"\n",
    "    dbutils.fs.put(raw_path + \"doc_events.json\", sample.strip(), overwrite=True)\n",
    "\n",
    "# -- 1c) define the ‚Äúas-is‚Äù schema\n",
    "bronze_schema = StructType([\n",
    "    StructField(\"event_id\",   StringType(), True),\n",
    "    StructField(\"user_id\",    StringType(), True),\n",
    "    StructField(\"doc_id\",     StringType(), True),\n",
    "    StructField(\"action\",     StringType(), True),\n",
    "    StructField(\"event_time\", StringType(), True),\n",
    "    StructField(\"device\", StructType([\n",
    "        StructField(\"os\",     StringType(), True),\n",
    "        StructField(\"region\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# -- 1d) read raw JSON\n",
    "df_raw = spark.read \\\n",
    "    .schema(bronze_schema) \\\n",
    "    .json(raw_path + \"doc_events.json\")\n",
    "\n",
    "# -- 1e) add audit columns\n",
    "df_bronze = df_raw \\\n",
    "    .withColumn(\"ingest_ts\", current_timestamp()) \\\n",
    "    .withColumn(\"batch_id\",   lit(process_id))\n",
    "\n",
    "\n",
    "\n",
    "# -- 1f) write to Bronze Delta (append-only)\n",
    "dbutils.fs.mkdirs(bronze_path)\n",
    "df_bronze.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(bronze_path)\n",
    "\n",
    "# -- verify\n",
    "print(\"üîç Bronze rows:\")\n",
    "display(spark.read.format(\"delta\").load(bronze_path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db5e7d6a-a607-49dd-a138-61515e331435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) SILVER ‚Äì clean, cast, dedupe, flatten & conform\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "from delta.tables           import DeltaTable\n",
    "import os\n",
    "\n",
    "# -- 2a) paths\n",
    "bronze_path = \"/tmp/bronze/doc_events/\"\n",
    "silver_path = \"/tmp/silver/doc_events/\"\n",
    "\n",
    "# -- 2b) read Bronze\n",
    "df = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "# -- 2c) basic cleaning & flatten\n",
    "df2 = (\n",
    "    df.filter(\"event_time IS NOT NULL\")\n",
    "      .withColumn(\"event_ts\",  to_timestamp(\"event_time\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
    "      .withColumn(\"os\",       col(\"device.os\"))\n",
    "      .withColumn(\"region\",   col(\"device.region\"))\n",
    "      .select(\n",
    "        \"event_id\",\"user_id\",\"doc_id\",\"action\",\n",
    "        \"event_ts\",\"os\",\"region\",\"ingest_ts\",\"batch_id\"\n",
    "      )\n",
    ")\n",
    "\n",
    "# -- 2d) de-duplicate on natural key\n",
    "df2 = df2.dropDuplicates([\"event_id\"])\n",
    "\n",
    "# -- 2e) write/upsert to Silver Delta\n",
    "if DeltaTable.isDeltaTable(spark, silver_path):\n",
    "    DeltaTable.forPath(spark, silver_path) \\\n",
    "      .merge(\n",
    "        df2.alias(\"new\"),\n",
    "        \"new.event_id = event_id\"\n",
    "      ) \\\n",
    "      .whenMatchedUpdateAll() \\\n",
    "      .whenNotMatchedInsertAll() \\\n",
    "      .execute()\n",
    "else:\n",
    "    os.makedirs(silver_path, exist_ok=True)\n",
    "    df2.write.format(\"delta\") \\\n",
    "       .mode(\"overwrite\") \\\n",
    "       .save(silver_path)\n",
    "\n",
    "# -- verify\n",
    "print(\"üîç Silver rows:\")\n",
    "display(spark.read.format(\"delta\").load(silver_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20275007-01a2-4f8a-a2d9-624f60fcf8a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3) GOLD ‚Äì aggregate into a consumption-ready fact table\n",
    "\n",
    "from pyspark.sql.functions import date_trunc, count\n",
    "\n",
    "# -- 3a) paths\n",
    "silver_path = \"/tmp/silver/doc_events/\"\n",
    "gold_path   = \"/tmp/gold/doc_events_usage/\"\n",
    "\n",
    "# -- 3b) read Silver\n",
    "df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# -- 3c) compute daily usage by doc\n",
    "gold = (\n",
    "    df_silver\n",
    "      .withColumn(\"day\", date_trunc(\"DAY\",\"event_ts\"))\n",
    "      .groupBy(\"day\",\"doc_id\")\n",
    "      .agg(\n",
    "         count(\"*\"   ).alias(\"n_events\"),\n",
    "         count(\"user_id\").alias(\"n_users\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# -- 3d) write to Gold Delta, partitioned by date\n",
    "dbutils.fs.mkdirs(gold_path)\n",
    "gold.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"day\") \\\n",
    "    .save(gold_path)\n",
    "\n",
    "# -- verify\n",
    "print(\"üîç Gold rows:\")\n",
    "display(spark.read.format(\"delta\").load(gold_path))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-08-05 20_00_24",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
