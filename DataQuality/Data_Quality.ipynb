{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc27b9d2-64ba-41b4-b7ec-a1b3e38176f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%pip install great_expectations\n",
    "import great_expectations as ge\n",
    "\n",
    "# 1) point at your Delta table (or DataFrame)\n",
    "df = spark.read.format(\"delta\").load(\"/tmp/bronze/covid_nyt\").toPandas()\n",
    "context = ge.get_context()\n",
    "\n",
    "# 2) create a new suite\n",
    "suite = context.create_expectation_suite(\"covid_quality\", overwrite_existing=True)\n",
    "\n",
    "# 3) turn your Pandas DF into a GE Dataset\n",
    "gedf = ge.from_pandas(df, expectation_suite_name=\"covid_quality\")\n",
    "\n",
    "# 4) add some expectations\n",
    "gedf.expect_column_values_to_not_be_null(\"date\")\n",
    "gedf.expect_column_values_to_be_between(\"cases\", min_value=0)\n",
    "gedf.expect_column_values_to_be_unique([\"date\",\"county\",\"state\"])\n",
    "\n",
    "# 5) run a checkpoint\n",
    "from great_expectations.checkpoint import SimpleCheckpoint\n",
    "checkpoint = SimpleCheckpoint(\n",
    "    name=\"covid_checkpoint\",\n",
    "    data_context=context,\n",
    "    batch_request=gedf.get_batch_request(),\n",
    "    expectation_suite_name=\"covid_quality\"\n",
    ")\n",
    "results = checkpoint.run()\n",
    "print(results.get_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbce761-2d39-430d-a229-589d3fb1f49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from great_expectations.dataset.sparkdf_dataset import SparkDFDataset\n",
    "\n",
    "# Load as SparkDF\n",
    "sdf = spark.table(\"bronze.covid_nyt\")\n",
    "\n",
    "# Wrap it\n",
    "gedf_spark = SparkDFDataset(sdf, expectation_suite_name=\"covid_quality\")\n",
    "\n",
    "# Declare the same expectations\n",
    "gedf_spark.expect_column_values_to_not_be_null(\"date\")\n",
    "gedf_spark.expect_column_values_to_be_between(\"cases\", min_value=0)\n",
    "gedf_spark.expect_column_values_to_be_unique([\"date\",\"county\",\"state\"])\n",
    "\n",
    "# Run validation directly\n",
    "results = gedf_spark.validate()\n",
    "print(results.get_statistics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f572e750-ecc5-4165-8ca7-0e304e0b791d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade \"great_expectations[spark]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d84969d-0029-46f6-b6b6-1e064931fc18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "\n",
    "# 1) Get (or initialize) your Data Context\n",
    "context = ge.get_context()\n",
    "\n",
    "# 2) Define a RuntimeBatchRequest pointing at your Spark table\n",
    "batch_request = RuntimeBatchRequest(\n",
    "    datasource_name=\"my_spark_datasource\",            # name of your Spark datasource in great_expectations.yml\n",
    "    data_connector_name=\"default_runtime_data_connector\",\n",
    "    data_asset_name=\"bronze_covid_nyt\",                # arbitrary internal name\n",
    "    runtime_parameters={\"query\": \"SELECT * FROM bronze.covid_nyt\"},\n",
    "    batch_identifiers={\"pipeline_stage\": \"bronze\"},\n",
    ")\n",
    "\n",
    "# Create the suite (wonâ€™t overwrite if it already exists)\n",
    "# Create (or update) the suite\n",
    "context.create_expectation_suite(\"covid_quality\")\n",
    "\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=\"covid_quality\"\n",
    ")\n",
    "\n",
    "# 4) Declare your expectations\n",
    "validator.expect_column_values_to_not_be_null(\"date\")\n",
    "validator.expect_column_values_to_be_between(\"cases\", min_value=0)\n",
    "validator.expect_column_values_to_be_unique([\"date\", \"county\", \"state\"])\n",
    "\n",
    "# 5) Execute validation\n",
    "results = validator.validate()\n",
    "print(results.get_statistics())\n",
    "\n",
    "#Otherwise I would have written the following code\n",
    "#SELECT * FROM bronze.covid_nyt WHERE date IS NULL;\n",
    "#SELECT * FROM bronze.covid_nyt WHERE cases < 0;\n",
    "#SELECT date, county, state, COUNT(*) FROM bronze.covid_nyt GROUP BY date, county, state HAVING COUNT(*) > 1;\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Quality",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
