{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "474ccd2c-fa28-4b60-8610-bc366a696ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 02_ACID Properties\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def list_parquet_files(directory):\n",
    "    print(f\"\\n=== Parquet files in: {directory} ===\")\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in sorted(files):\n",
    "            if file.endswith(\".parquet\"):\n",
    "                print(os.path.join(root, file))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "path  = \"/local_disk0/tmp/delta_acid_demo\"\n",
    "\n",
    "# ── Step A: VERSION 0 ──\n",
    "# Create a brand-new Delta table with 5 rows\n",
    "spark.range(0, 5) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .save(path)\n",
    "\n",
    "list_parquet_files(path)\n",
    "\n",
    "# ── Step B: VERSION 1 ──\n",
    "# Append 5 more rows\n",
    "spark.range(5, 10) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"append\") \\\n",
    "     .save(path)\n",
    "\n",
    "list_parquet_files(path)\n",
    "\n",
    "# Step C: VERSION 2 ──\n",
    "deltaTable = DeltaTable.forPath(spark, path)\n",
    "deltaTable.update(\n",
    "     condition = \"id == 2\",\n",
    "     set = {\"id\": \"200\"}\n",
    ")\n",
    "list_parquet_files(path)\n",
    "\n",
    "print (\"History\")\n",
    "spark.sql(f\"DESCRIBE HISTORY delta.`{path}`\").show(truncate=False)\n",
    "\n",
    "print(\"-version 0\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\",0).load(path).show()\n",
    "\n",
    "print(\"-version 1\")\n",
    "spark.read.format(\"delta\").option(\"versionAsOf\",1).load(path).show()  \n",
    "\n",
    "print(\" Latest version\")\n",
    "spark.read.format(\"delta\").load(path).show()  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9dc2fd7b-8d72-44a1-bb8b-06ebf7fb425b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scratch\n",
    "# Paths for the Delta table and the streaming checkpoint\n",
    "delta_path = \"dbfs:/FileStore/delta_unified_demo\"\n",
    "checkpoint = \"dbfs:/FileStore/delta_unified_demo_checkpoint\"\n",
    "\n",
    "# Clean up old data if present\n",
    "dbutils.fs.rm(delta_path, recurse=True)\n",
    "dbutils.fs.rm(checkpoint, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0f0d065e-e2f6-4a84-be59-b5bc67fb7ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scratch\n",
    "# CELL 2: BATCH WRITE\n",
    "# Write IDs 0–4 in one batch\n",
    "spark.range(0, 5) \\\n",
    "     .write \\\n",
    "     .format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .save(delta_path)\n",
    "\n",
    "print(\"Batch data:\")\n",
    "display(spark.read.format(\"delta\").load(delta_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "71e2aedc-1a50-497f-a6e2-569449ed8498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Checkpoint demo - \n",
    "#Can skip - since there is one more code sample one below \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "delta_path = \"dbfs:/FileStore/delta_stream_demo\"\n",
    "checkpoint =  \"dbfs:/FileStore/delta_stream_demo_ckpt\"\n",
    "\n",
    "# 1) start a streaming write: \n",
    "stream_df = (\n",
    "    spark.readStream.format(\"rate\")\n",
    "         .option(\"rowsPerSecond\", 10)   # 10 events/sec\n",
    "         .load()\n",
    "         .selectExpr(\"value AS event_id\")\n",
    ")\n",
    "\n",
    "stream_query = (\n",
    "    stream_df\n",
    "      .writeStream\n",
    "      .format(\"delta\")\n",
    "      .option(\"checkpointLocation\", checkpoint)\n",
    "      .outputMode(\"append\")\n",
    "      .trigger(availableNow=True)\n",
    "      .start(delta_path)\n",
    ")\n",
    "\n",
    "# Let it run for a few seconds, then stop\n",
    "import time; time.sleep(5)\n",
    "stream_query.stop()\n",
    "\n",
    "# 2) Inspect the files & log\n",
    "print(\"Files on disk:\")\n",
    "display(dbutils.fs.ls(delta_path))\n",
    "print(\"\\nDelta log entries:\")\n",
    "display(dbutils.fs.ls(delta_path + \"/_delta_log\"))\n",
    "print(\"\\nLatest snapshot:\")\n",
    "display(spark.read.format(\"delta\").load(delta_path).orderBy(\"event_id\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_ACID_properties",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
