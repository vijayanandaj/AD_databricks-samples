{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6976674-222b-4e4d-8dcf-a4b8b6edd782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Bronze: just land raw data into Delta\n",
    "# -----------------------------------------------------------------------------\n",
    "@dlt.table(\n",
    "    name=\"bronze_events\",\n",
    "    comment=\"Raw ingested data; exactly the files you pointed at in 01_Bronze.py\"\n",
    ")\n",
    "def bronze_events():\n",
    "    # Copy your spark.read() + write code from 01_Bronze.py here…\n",
    "    df = (\n",
    "        spark.read\n",
    "             .format(\"json\")           # or parquet — whatever your sample does\n",
    "             .load(\"/mnt/raw/source_path\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Silver: flatten, dedupe, conform, etc.\n",
    "# -----------------------------------------------------------------------------\n",
    "@dlt.table(\n",
    "    name=\"silver_events\",\n",
    "    comment=\"Flattened structs, deduplicated, with feature_category joined on\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_event_ts\", \"event_ts IS NOT NULL\")   # example expectation\n",
    "def silver_events():\n",
    "    bronze = dlt.read(\"bronze_events\")\n",
    "\n",
    "    # 2.a) flatten nested structs (if any)\n",
    "    # Copy your flatten logic: e.g.\n",
    "    flat = (\n",
    "        bronze\n",
    "        .select(\n",
    "            col(\"user_id\"),\n",
    "            col(\"event.app_name\").alias(\"app_name\"),\n",
    "            col(\"event.event_type\").alias(\"event_type\"),\n",
    "            col(\"event.event_ts\").alias(\"event_ts\"),\n",
    "            *[c for c in bronze.columns if c not in (\"event\",)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2.b) deduplicate on natural key\n",
    "    deduped = flat.dropDuplicates(\n",
    "        [\"user_id\",\"app_name\",\"event_type\",\"event_ts\"]\n",
    "    )\n",
    "\n",
    "    # 2.c) conformance join against small lookup table\n",
    "    feature_cat = (\n",
    "        spark.table(\"feature_catalog\")  # or read the small lookup from Bronze\n",
    "    )\n",
    "    silver = (\n",
    "        deduped\n",
    "        .join(\n",
    "            feature_cat,\n",
    "            on=[\"app_name\",\"event_type\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return silver\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Gold: aggregates, roll-ups, business-level tables\n",
    "# -----------------------------------------------------------------------------\n",
    "@dlt.table(\n",
    "    name=\"gold_event_summary\",\n",
    "    comment=\"Business-level summary of events per user/app/…\"\n",
    ")\n",
    "def gold_event_summary():\n",
    "    silver = dlt.read(\"silver_events\")\n",
    "\n",
    "    gold = (\n",
    "        silver\n",
    "        .groupBy(\"user_id\", \"feature_category\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"event_count\"),\n",
    "            min(\"event_ts\").alias(\"first_seen\"),\n",
    "            max(\"event_ts\").alias(\"last_seen\")\n",
    "        )\n",
    "    )\n",
    "    return gold\n",
    "\n",
    "Key points:\n",
    "\t1.\tReplace each notebook’s body\n",
    "Copy your ingestion code from 01_Bronze.py into bronze_events().\n",
    "Copy your cleaning/flatten/dedupe/join from 02_silver.ipynb into silver_events().\n",
    "Copy your aggregations from 03_Gold.ipynb into gold_event_summary().\n",
    "\t2.\tDeclare each stage with @dlt.table\n",
    "That tells Delta Live Tables how to build each layer in order.\n",
    "\t3.\tRemove your old driver\n",
    "You no longer need the run_notebook() orchestration — DLT takes over scheduling and execution.\n",
    "\t4.\tAttach it to a DLT Pipeline\n",
    "\t•\tIn the UI, go to Jobs & Pipelines → Delta Live Tables\n",
    "\t•\tCreate a new DLT pipeline and point it at your Medallion_DLT.py file in /Workspace/.../Medallion_DLT.py\n",
    "\t•\tConfigure the cluster/compute you want it to run on\n",
    "\t•\tClick Start\n",
    "From then on, DLT will execute your three stages in order, manage lineage, enforce expectations, etc."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT_template_excercise",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
