{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd3816d-3939-4442-abb7-3d9b8322eb21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TEST THIS?\n",
    "\n",
    "# covid_lakehouse_.py\n",
    "# -------------------------------------------------------------------\n",
    "# End-to-end COVID Lakehouse demo \n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 1: Bootstrap Spark + Hive Metastore & Prep Workspace\n",
    "# (from new1_Initi.py)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pyspark.sql import SparkSession\n",
    "import shutil, os\n",
    "\n",
    "# 1) Launch Spark with Hive support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"COVID_Lakehouse_COVIDExercise\")\n",
    "      .enableHiveSupport()\n",
    "      .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2) Create / switch to our covid database\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS covid\")\n",
    "spark.sql(\"USE covid\")\n",
    "\n",
    "# 3) Define and clean our exercise folder (all in DBFS!)\n",
    "base = \"dbfs:/tmp/COVID_exercise\"\n",
    "dbutils.fs.rm(base, recurse=True)\n",
    "\n",
    "# 4) Confirmation\n",
    "print(f\"âœ… Workspace initialized under {base}\")\n",
    "print(\"   Hive database 'covid' is ready, Spark session active.\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 2: Read real CSV files & show samples, then Bronze ingestion\n",
    "# (from New2_Bronze.py)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pyspark.sql.functions import lit, col, current_timestamp, to_date\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "data_dir = \"dbfs:/databricks-datasets/COVID/covid-19-data\"\n",
    "# Use the same base defined above\n",
    "bronze_counties_path = f\"{base}/bronze_counties\"\n",
    "bronze_recent_path   = f\"{base}/bronze_recent\"\n",
    "bronze_states_path   = f\"{base}/bronze_states\"\n",
    "\n",
    "# 1) Read each CSV with schema inference\n",
    "counties_df = spark.read.option(\"header\",True).option(\"inferSchema\",True) \\\n",
    "                    .csv(f\"{data_dir}/us-counties.csv\")\n",
    "recent_df   = spark.read.option(\"header\",True).option(\"inferSchema\",True) \\\n",
    "                    .csv(f\"{data_dir}/us-counties-recent.csv\")\n",
    "states_df   = spark.read.option(\"header\",True).option(\"inferSchema\",True) \\\n",
    "                    .csv(f\"{data_dir}/us-states.csv\")\n",
    "\n",
    "# 2) Show samples\n",
    "print(\"ðŸ‘‰ us-counties.csv sample:\")\n",
    "counties_df.show(5, truncate=False); counties_df.printSchema()\n",
    "print(\"ðŸ‘‰ us-counties-recent.csv sample:\")\n",
    "recent_df.show(5, truncate=False);   recent_df.printSchema()\n",
    "print(\"ðŸ‘‰ us-states.csv sample:\")\n",
    "states_df.show(5, truncate=False);   states_df.printSchema()\n",
    "\n",
    "# 3) Bronze | counties\n",
    "dbutils.fs.rm(bronze_counties_path, recurse=True)\n",
    "spark.sql(\"DROP TABLE IF EXISTS bronze_counties\")\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE bronze_counties (\n",
    "    date        DATE,\n",
    "    county      STRING,\n",
    "    state       STRING,\n",
    "    fips        INT,\n",
    "    cases       INT,\n",
    "    deaths      INT,\n",
    "    source      STRING,\n",
    "    ingest_ts   TIMESTAMP\n",
    "  ) USING DELTA\n",
    "  LOCATION '{bronze_counties_path}'\n",
    "\"\"\")\n",
    "counties_df \\\n",
    "  .withColumn(\"date\", to_date(\"date\")) \\\n",
    "  .withColumn(\"source\", lit(\"counties_full\")) \\\n",
    "  .withColumn(\"ingest_ts\", current_timestamp()) \\\n",
    "  .write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(bronze_counties_path)\n",
    "\n",
    "print(\"âœ… bronze_counties ready\")\n",
    "spark.table(\"bronze_counties\").show(5, truncate=False)\n",
    "\n",
    "# 4) Bronze | recent\n",
    "dbutils.fs.rm(bronze_recent_path, recurse=True)\n",
    "spark.sql(\"DROP TABLE IF EXISTS bronze_recent\")\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE bronze_recent (\n",
    "    date        DATE,\n",
    "    county      STRING,\n",
    "    state       STRING,\n",
    "    fips        INT,\n",
    "    cases       INT,\n",
    "    deaths      INT,\n",
    "    source      STRING,\n",
    "    ingest_ts   TIMESTAMP\n",
    "  ) USING DELTA\n",
    "  LOCATION '{bronze_recent_path}'\n",
    "\"\"\")\n",
    "recent_df \\\n",
    "  .withColumn(\"date\", to_date(\"date\")) \\\n",
    "  .withColumn(\"source\", lit(\"counties_recent\")) \\\n",
    "  .withColumn(\"ingest_ts\", current_timestamp()) \\\n",
    "  .write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(bronze_recent_path)\n",
    "\n",
    "print(\"âœ… bronze_recent ready\")\n",
    "spark.table(\"bronze_recent\").show(5, truncate=False)\n",
    "\n",
    "# 5) Bronze | states\n",
    "dbutils.fs.rm(bronze_states_path, recurse=True)\n",
    "spark.sql(\"DROP TABLE IF EXISTS bronze_states\")\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE bronze_states (\n",
    "    date        DATE,\n",
    "    state       STRING,\n",
    "    fips        INT,\n",
    "    cases       INT,\n",
    "    deaths      INT,\n",
    "    source      STRING,\n",
    "    ingest_ts   TIMESTAMP\n",
    "  ) USING DELTA\n",
    "  LOCATION '{bronze_states_path}'\n",
    "\"\"\")\n",
    "states_df \\\n",
    "  .withColumn(\"date\", to_date(\"date\")) \\\n",
    "  .withColumn(\"source\", lit(\"states\")) \\\n",
    "  .withColumn(\"ingest_ts\", current_timestamp()) \\\n",
    "  .write.format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(bronze_states_path)\n",
    "\n",
    "print(\"âœ… bronze_states ready\")\n",
    "spark.table(\"bronze_states\").show(5, truncate=False)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 3: Create Silver 1 & checkpoint table\n",
    "# (from new3_Silver1.py)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "silver1_path = f\"{base}/silver_county\"\n",
    "dbutils.fs.rm(silver1_path, recurse=True)\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver_county\")\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE silver_county (\n",
    "    report_date DATE,\n",
    "    county      STRING,\n",
    "    state       STRING,\n",
    "    cases       INT,\n",
    "    deaths      INT,\n",
    "    source      STRING,\n",
    "    ingest_ts   TIMESTAMP\n",
    "  ) USING DELTA\n",
    "  PARTITIONED BY (state)\n",
    "  LOCATION '{silver1_path}'\n",
    "\"\"\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver_county_ckpt\")\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE silver_county_ckpt (\n",
    "    last_ts TIMESTAMP\n",
    "  ) USING DELTA\n",
    "\"\"\")\n",
    "print(\"âœ… silver_county + silver_county_ckpt created\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 4: Define the upsert function\n",
    "# (from new4_Func.py)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit, col, to_date\n",
    "def process_silver_county():\n",
    "    # 1) Read last watermark (as Python datetime)\n",
    "    ck = spark.table(\"silver_county_ckpt\").collect()\n",
    "    last_ts = ck[0][\"last_ts\"] if ck and ck[0][\"last_ts\"] else datetime(1970,1,1)\n",
    "\n",
    "    # 2) Union Bronze full + recent\n",
    "    bronze_full   = spark.table(\"bronze_counties\")\n",
    "    bronze_recent = spark.table(\"bronze_recent\")\n",
    "    bronze_union  = bronze_full.unionByName(bronze_recent)\n",
    "\n",
    "    # 3) Pull only new rows\n",
    "    new_df = bronze_union.filter(col(\"ingest_ts\") > lit(last_ts)) if last_ts else bronze_union\n",
    "\n",
    "    # 4) Clean & cast\n",
    "    clean_df = (\n",
    "      new_df\n",
    "        .filter((col(\"county\").isNotNull()) & (col(\"cases\") >= 0))\n",
    "        .withColumn(\"report_date\", to_date(col(\"date\")))\n",
    "        .select(\"report_date\",\"county\",\"state\",\"cases\",\"deaths\",\"source\",\"ingest_ts\")\n",
    "    )\n",
    "\n",
    "    # 5) MERGE into Silver\n",
    "    silver = DeltaTable.forName(spark, \"silver_county\")\n",
    "    (\n",
    "      silver.alias(\"t\")\n",
    "            .merge(clean_df.alias(\"s\"),\n",
    "                   \"t.report_date = s.report_date AND t.county = s.county\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "    )\n",
    "\n",
    "    # 6) Advance watermark\n",
    "    max_ts = clean_df.agg({\"ingest_ts\":\"max\"}).collect()[0][0]\n",
    "    spark.createDataFrame([(max_ts,)], [\"last_ts\"]) \\\n",
    "         .write.format(\"delta\").mode(\"overwrite\") \\\n",
    "         .saveAsTable(\"silver_county_ckpt\")\n",
    "    print(f\"âœ… Silver County upserted. New watermark = {max_ts}\")\n",
    "\n",
    "# Run the function once\n",
    "process_silver_county()\n",
    "spark.table(\"silver_county\").show(5, truncate=False)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 5: Build Silver 2 & Gold tables\n",
    "# (from new5_Silver2_Gold.py)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pyspark.sql.functions import sum, window, avg, sum as _sum, date_trunc, col\n",
    "\n",
    "# Silver 2: daily state roll-up\n",
    "silver2_path = f\"{base}/silver_state_daily\"\n",
    "dbutils.fs.rm(silver2_path, recurse=True)\n",
    "spark.sql(\"DROP TABLE IF EXISTS silver_state_daily\")\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE silver_state_daily (\n",
    "    report_date  DATE,\n",
    "    state        STRING,\n",
    "    total_cases  LONG,\n",
    "    total_deaths LONG\n",
    "  ) USING DELTA\n",
    "  PARTITIONED BY (report_date)\n",
    "  LOCATION '{silver2_path}'\n",
    "\"\"\")\n",
    "(spark.table(\"silver_county\")\n",
    "      .groupBy(\"report_date\",\"state\")\n",
    "      .agg(\n",
    "        _sum(\"cases\").alias(\"total_cases\"),\n",
    "        _sum(\"deaths\").alias(\"total_deaths\")\n",
    "      )\n",
    "      .write.format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"overwriteSchema\",\"true\")\n",
    "      .save(silver2_path)\n",
    ")\n",
    "print(\"âœ… silver_state_daily built\")\n",
    "spark.table(\"silver_state_daily\").show(10, truncate=False)\n",
    "\n",
    "# Gold 1: 7-day moving average\n",
    "gold1_path = f\"{base}/gold_state_ma\"\n",
    "dbutils.fs.rm(gold1_path, recurse=True)\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold_state_ma\")\n",
    "gold1_df = (\n",
    "    spark.table(\"silver_state_daily\")\n",
    "         .withWatermark(\"report_date\", \"1 day\")\n",
    "         .groupBy(\n",
    "             col(\"state\"),\n",
    "             window(col(\"report_date\"), \"7 days\", \"1 day\").alias(\"w\")\n",
    "         )\n",
    "         .agg(\n",
    "             avg(\"total_cases\").alias(\"avg_cases\"),\n",
    "             avg(\"total_deaths\").alias(\"avg_deaths\")\n",
    "         )\n",
    "         .selectExpr(\"state\",\"w.end AS report_date\",\"avg_cases\",\"avg_deaths\")\n",
    ")\n",
    "gold1_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\",\"true\") \\\n",
    "    .saveAsTable(\"gold_state_ma\")\n",
    "print(\"âœ… gold_state_ma built\")\n",
    "spark.table(\"gold_state_ma\").show(5, truncate=False)\n",
    "\n",
    "# Gold 2: weekly summary\n",
    "gold2_path = f\"{base}/gold_weekly_summary\"\n",
    "dbutils.fs.rm(gold2_path, recurse=True)\n",
    "spark.sql(\"DROP TABLE IF EXISTS gold_weekly_summary\")\n",
    "gold2_df = (\n",
    "    spark.table(\"silver_state_daily\")\n",
    "         .withColumn(\"week_start\",date_trunc(\"week\",col(\"report_date\")))\n",
    "         .groupBy(\"state\",\"week_start\")\n",
    "         .agg(\n",
    "            _sum(\"total_cases\").alias(\"total_cases\"),\n",
    "            _sum(\"total_deaths\").alias(\"total_deaths\")\n",
    "         )\n",
    ")\n",
    "gold2_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\",\"true\") \\\n",
    "    .saveAsTable(\"gold_weekly_summary\")\n",
    "print(\"âœ… gold_weekly_summary built\")\n",
    "spark.table(\"gold_weekly_summary\") \\\n",
    "     .orderBy(\"week_start\",\"state\") \\\n",
    "     .show(5, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Exercise_2_Soln",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
