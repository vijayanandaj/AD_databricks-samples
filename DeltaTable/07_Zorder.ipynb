{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fcd3aa57-59a0-4074-ba4d-b4ae091eba89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#06 Zorder \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 1) Build a “wide” table: 1M rows, random country & age\n",
    "import random\n",
    "data = [(i,\n",
    "         random.choice([\"US\",\"CA\",\"MX\",\"IN\",\"DE\"]),\n",
    "         random.randint(1,100))\n",
    "        for i in range(1_000_000)]\n",
    "df = spark.createDataFrame(data, schema=[\"id\",\"country\",\"age\"])\n",
    "\n",
    "# 2) Write it as Delta (no partitioning)\n",
    "path = \"/tmp/delta_skip_zorder\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "\n",
    "# Read before Z-order\n",
    "# simple filter on age\n",
    "filtered = spark.read.format(\"delta\") \\\n",
    "    .load(path) \\\n",
    "    .filter(\"age BETWEEN 30 AND 40\")\n",
    "\n",
    "# show that many files are scanned\n",
    "print(\"Files scanned before ZORDER:\")\n",
    "filtered.explain(True)\n",
    "\n",
    "# Now cluster\n",
    "spark.sql(f\"OPTIMIZE delta.`{path}` ZORDER BY age\")\n",
    "\n",
    "# Read after Z-order\n",
    "filtered2 = spark.read.format(\"delta\") \\\n",
    "    .load(path) \\\n",
    "    .filter(\"age BETWEEN 30 AND 40\")\n",
    "print(\"Files scanned after ZORDER:\")\n",
    "filtered2.explain(True)\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "filtered.count()\n",
    "print(\"Before:\", time.time() - t0, \"s\")\n",
    "\n",
    "t1 = time.time()\n",
    "filtered2.count()\n",
    "print(\"After ZORDER:\", time.time() - t1, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45ad039d-470c-4924-b7bf-8d7bf314a6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "import time\n",
    "import random                         # ← add this\n",
    "from datetime import date, timedelta\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————\n",
    "# 1) Initialize Spark\n",
    "# ——————————————————————————————————————————————————————————\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"ZOrder_BeforeAfter_Demo\")\n",
    "      .enableHiveSupport()\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "# Base paths & table names\n",
    "BASE     = \"/tmp/zorder_demo\"\n",
    "TABLE_A  = \"demo.scenario_a\"    # low-cardinality test\n",
    "TABLE_B  = \"demo.scenario_b\"    # high-cardinality test\n",
    "\n",
    "# Shared start date for data generation\n",
    "start = date(2025, 1, 1)\n",
    "\n",
    "# Utility to print EXPLAIN ANALYZE\n",
    "def show_plan(sql):\n",
    "    print(f\"\\n=== PLAN: {sql.strip()} ===\")\n",
    "    print(spark.sql(f\"EXPLAIN ANALYZE {sql}\").collect()[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a82c5632-6c84-4841-878f-429fb2095c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Low‐Cardinality Region\n",
    "# Simulate ~1M rows with only 5 distinct regions over 30 dates\n",
    "import random\n",
    "from datetime import date, timedelta\n",
    "\n",
    "regions = [\"North\",\"South\",\"East\",\"West\",\"Central\"]\n",
    "start = date(2025,1,1)\n",
    "data = []\n",
    "for i in range(1_000_000):\n",
    "    d = start + timedelta(days=random.randint(0,29))\n",
    "    data.append((random.choice(regions), d.isoformat(), random.random()))\n",
    "\n",
    "dfA = spark.createDataFrame(data, schema=[\"region\",\"event_date\",\"value\"])\n",
    "\n",
    "# Write as Delta partitioned by event_date\n",
    "dfA.write.format(\"delta\") \\\n",
    "   .mode(\"overwrite\") \\\n",
    "   .partitionBy(\"event_date\") \\\n",
    "   .save(f\"{BASE}/scenario_a\")\n",
    "\n",
    "# Register in HMS\n",
    "# 1) Tell Spark/SQL to use the Hive Metastore catalog\n",
    "spark.sql(\"USE CATALOG spark_catalog\")\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo\")\n",
    "spark.sql(f\"\"\"\n",
    "  DROP TABLE IF EXISTS {TABLE_A};\n",
    "  CREATE TABLE {TABLE_A}\n",
    "  USING DELTA\n",
    "  LOCATION '{BASE}/scenario_a'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53202bde-cadb-47c2-926a-32e5b7730cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a representative filter: one region over a week\n",
    "queryA = \"\"\"\n",
    "SELECT sum(value) \n",
    "  FROM demo.scenario_a\n",
    " WHERE region = 'North'\n",
    "   AND event_date BETWEEN '2025-01-10' AND '2025-01-16'\n",
    "\"\"\"\n",
    "\n",
    "# Before Z-order\n",
    "show_plan(queryA)\n",
    "\n",
    "# Optimize & Z-Order\n",
    "spark.sql(f\"OPTIMIZE {TABLE_A} ZORDER BY (region, event_date)\")\n",
    "\n",
    "# After Z-order\n",
    "show_plan(queryA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6005a005-fee1-4935-80f2-92ef07f1a80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " #High‐Cardinality User ID\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "import time\n",
    "\n",
    "# ——————————————————————————————————————————————————————————\n",
    "# 1) Initialize Spark\n",
    "# ——————————————————————————————————————————————————————————\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "      .appName(\"ZOrder_BeforeAfter_Demo\")\n",
    "      .enableHiveSupport()\n",
    "      .getOrCreate()\n",
    ")\n",
    "\n",
    "# Base paths\n",
    "BASE = \"dbfs:/tmp/zorder_demo\"\n",
    "TABLE_B = \"demo.scenario_b\"\n",
    "\n",
    "# Make sure `start` is defined for both scenarios\n",
    "from datetime import date, timedelta\n",
    "start = date(2025, 1, 1)\n",
    "\n",
    "# Utility to print EXPLAIN ANALYZE\n",
    "def show_plan(sql):\n",
    "    print(f\"\\n=== PLAN: {sql.strip()} ===\")\n",
    "    print(spark.sql(f\"EXPLAIN ANALYZE {sql}\").collect()[0][0])\n",
    "\n",
    "\n",
    "# ——————————————————————————————————————————————————————————\n",
    "# 2B) Build & Register Scenario B (High-Cardinality)\n",
    "# ——————————————————————————————————————————————————————————\n",
    "# Simulate ~1M rows with 50,000 distinct user_ids over 30 dates\n",
    "users = [f\"user_{i}\" for i in range(50_000)]\n",
    "data = []\n",
    "for i in range(1_000_000):\n",
    "    d = start + timedelta(days=random.randint(0,29))\n",
    "    data.append((random.choice(users), d.isoformat(), random.random()))\n",
    "\n",
    "dfB = spark.createDataFrame(data, schema=[\"user_id\",\"event_date\",\"value\"])\n",
    "\n",
    "# Write as Delta partitioned by event_date\n",
    "dfB.write.format(\"delta\") \\\n",
    "   .mode(\"overwrite\") \\\n",
    "   .partitionBy(\"event_date\") \\\n",
    "   .save(f\"{BASE}/scenario_b\")\n",
    "\n",
    "# Register in HMS\n",
    "# 1) Tell Spark/SQL to use the Hive Metastore catalog\n",
    "spark.sql(\"USE CATALOG spark_catalog\")\n",
    "# Use a fully qualified DBFS path\n",
    "BASE = \"dbfs:/tmp/zorder_demo\"\n",
    "# Make sure the database exists\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo\")\n",
    "\n",
    "# Drop any old table\n",
    "spark.sql(\"DROP TABLE IF EXISTS demo.scenario_b\")\n",
    "\n",
    "# Create the new Delta table\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE demo.scenario_b (\n",
    "    user_id STRING,\n",
    "    event_date DATE,\n",
    "    value DOUBLE\n",
    "  )\n",
    "  USING DELTA\n",
    "  LOCATION '{BASE}/scenario_b'\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Zorder",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
