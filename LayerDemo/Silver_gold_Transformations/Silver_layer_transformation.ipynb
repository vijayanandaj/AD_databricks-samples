{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "743bcf6d-4dcc-459b-8ef4-edd7f8410efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Silver layer transformation \n",
    "# dedup sample \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Initial batch for July 31\n",
    "df1 = spark.createDataFrame(\n",
    "  [(“2025-07-31”,”CA”,100)],\n",
    "  [\"event_date\",\"region\",\"cases\"]\n",
    ")\n",
    "\n",
    "# Late arrival batch\n",
    "df2 = spark.createDataFrame(\n",
    "  [(“2025-07-31”,”CA”,100)],\n",
    "  [\"event_date\",\"region\",\"cases\"]\n",
    ")\n",
    "\n",
    "# Naïve append (duplicates!)\n",
    "df_naive = df1.union(df2)\n",
    "print(\"Naïve count:\", df_naive.count())   # 2\n",
    "\n",
    "# With dedup on natural key\n",
    "df_clean = df_naive.dropDuplicates([\"event_date\",\"region\"])\n",
    "print(\"Deduped count:\", df_clean.count()) # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b540760c-1335-41e6-ad49-f619fc194885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 0) Build a toy “raw” DataFrame\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "raw = spark.createDataFrame([\n",
    "    (1, \" Alice  \", \"2025-08-01\", {\"os\":\"iOS\",\"region\":\" CA \"}, \"view_home\",  \"100\",   \"0\", \"secret@example.com\" ),\n",
    "    (2, \" Bob\",      None,      {\"os\":\"Android\",\"region\":\"NY\"},  \"addToCart\",\"200\",  None,  \"bob@ex.com\"        ),\n",
    "    (2, \"Bob \",      \"2025-08-01\",{\"os\":\"Android\",\"region\":\"NY\"},\"addToCart\",\"200\",  \"5\",   \"bob@ex.com\"        )\n",
    "], schema=StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"user_name\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"device\", StructType([\n",
    "        StructField(\"os\", StringType()), StructField(\"region\", StringType())\n",
    "    ])),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"cases_str\", StringType()),\n",
    "    StructField(\"deaths_str\", StringType()),\n",
    "    StructField(\"email\", StringType())\n",
    "]))\n",
    "\n",
    "print(\"=== raw ===\")\n",
    "display(raw)\n",
    "\n",
    "\n",
    "# 1) Filter out bad rows (drop rows missing date)\n",
    "step1 = raw.filter(col(\"date\").isNotNull())\n",
    "print(\"=== step1: filter date IS NOT NULL ===\")\n",
    "display(step1)\n",
    "\n",
    "\n",
    "# 2) Type-cast & rename\n",
    "step2 = step1.select(\n",
    "    to_date(\"date\", \"yyyy-MM-dd\").alias(\"event_date\"),\n",
    "    col(\"user_id\"),\n",
    "    col(\"event_type\"),\n",
    "    col(\"cases_str\").cast(\"int\").alias(\"cases\"),\n",
    "    col(\"deaths_str\").cast(\"int\").alias(\"deaths\"),\n",
    "    col(\"device\"),\n",
    "    col(\"email\")\n",
    ")\n",
    "print(\"=== step2: cast & rename ===\")\n",
    "display(step2)\n",
    "\n",
    "\n",
    "# 3) Drop unused columns (we drop email)\n",
    "step3 = step2.drop(\"email\")\n",
    "print(\"=== step3: drop unused (email) ===\")\n",
    "display(step3)\n",
    "\n",
    "\n",
    "# 4) Flatten nested structs (device → os, region)\n",
    "step4 = step3.select(\n",
    "    \"event_date\",\"user_id\",\"event_type\",\"cases\",\"deaths\",\n",
    "    col(\"device.os\").alias(\"os\"),\n",
    "    col(\"device.region\").alias(\"region\")\n",
    ")\n",
    "print(\"=== step4: flatten device struct ===\")\n",
    "display(step4)\n",
    "\n",
    "\n",
    "# 5) Normalize column names (skip since already snake_case)\n",
    "\n",
    "\n",
    "# 6) Trim & clean strings (region)\n",
    "step6 = step4.withColumn(\"region\", trim(lower(col(\"region\"))))\n",
    "print(\"=== step6: trim & lower region ===\")\n",
    "display(step6)\n",
    "\n",
    "\n",
    "# 7) Drop duplicates on natural key (event_date, user_id, event_type)\n",
    "step7 = step6.dropDuplicates([\"event_date\",\"user_id\",\"event_type\"])\n",
    "print(\"=== step7: dropDuplicates on (event_date,user_id,event_type) ===\")\n",
    "display(step7)\n",
    "\n",
    "\n",
    "# 8) Surrogate key generation\n",
    "step8 = step7.withColumn(\"event_id\",\n",
    "    sha2(concat_ws(\"|\",\"event_date\",\"user_id\",\"event_type\"),256)\n",
    ")\n",
    "print(\"=== step8: add event_id surrogate key ===\")\n",
    "display(step8)\n",
    "\n",
    "\n",
    "# 9) Derive new columns (event_hour)\n",
    "step9 = step8.withColumn(\"event_hour\", hour(col(\"event_date\")))\n",
    "print(\"=== step9: add event_hour ===\")\n",
    "display(step9)\n",
    "\n",
    "\n",
    "# 10) Conformance join\n",
    "lookup = spark.createDataFrame(\n",
    "    [(\"view_home\",\"Browsing\"),(\"addtocart\",\"ShoppingCart\")],\n",
    "    [\"event_type\",\"feature_category\"]\n",
    ")\n",
    "step10 = step9.join(lookup, on=\"event_type\", how=\"left\")\n",
    "print(\"=== step10: conformance join to lookup ===\")\n",
    "display(step10)\n",
    "\n",
    "\n",
    "# 11) Mask or obfuscate PII (email hash)\n",
    "# Re-attach email column for masking\n",
    "step10_with_email = step10.withColumn(\"email\",\n",
    "    when(col(\"user_id\")==1, lit(\"secret@example.com\"))\n",
    "    .otherwise(lit(\"bob@ex.com\"))\n",
    ")\n",
    "step11 = step10_with_email.withColumn(\"email_hashed\", sha2(col(\"email\"),256)).drop(\"email\")\n",
    "print(\"=== step11: mask PII (email_hashed) ===\")\n",
    "display(step11)\n",
    "\n",
    "\n",
    "# 12) Fill defaults / NULL handling\n",
    "step12 = step11.na.fill({\"deaths\":0})\n",
    "print(\"=== step12: fill null deaths → 0 ===\")\n",
    "display(step12)\n",
    "\n",
    "\n",
    "# 13) Partition / repartition by event_date\n",
    "step13 = step12.repartition(col(\"event_date\"))\n",
    "print(\"=== step13: repartition by event_date (no visible diff) ===\")\n",
    "display(step13)\n",
    "\n",
    "\n",
    "# 14) File-size tuning (coalesce) — no visible change in output\n",
    "step14 = step13.coalesce(1)\n",
    "print(\"=== step14: coalesce to 1 partition (no visible diff) ===\")\n",
    "display(step14)\n",
    "\n",
    "\n",
    "# 15) Quality assertions / expectations\n",
    "bad = step14.filter(col(\"cases\") < 0)\n",
    "print(\"=== step15: bad rows count (cases < 0) ===\", bad.count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_layer_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
