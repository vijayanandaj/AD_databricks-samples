{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "836f37ec-5457-48cb-b27c-4939f07fa7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Bronze\n",
    "# 1) BRONZE – land “as-is” JSON into a Delta table, append-only\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "\n",
    "# TODO 1: pick a raw → bronze path naming convention\n",
    "raw_path    = \"/tmp/raw/doc_events/\"\n",
    "bronze_path = \"/tmp/bronze/doc_events/\"\n",
    "\n",
    "# TODO 2: decide: rm existing bronze folder on rerun?  (uncomment if yes)\n",
    "# dbutils.fs.rm(bronze_path, recurse=True)\n",
    "\n",
    "# TODO 3: define the “as-is” schema for your JSON source\n",
    "bronze_schema = StructType([\n",
    "  StructField(\"event_id\",   StringType(), True),\n",
    "  StructField(\"user_id\",    StringType(), True),\n",
    "  StructField(\"doc_id\",     StringType(), True),\n",
    "  StructField(\"action\",     StringType(), True),\n",
    "  StructField(\"event_time\", StringType(), True),\n",
    "  StructField(\"device\", StructType([\n",
    "    StructField(\"os\",     StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "  ]), True)\n",
    "])\n",
    "\n",
    "# TODO 4: read the raw JSON (hint: spark.read.schema(...).json(...))\n",
    "df_raw = spark.read \\\n",
    "  .schema(bronze_schema) \\\n",
    "  .json(raw_path + \"doc_events.json\")\n",
    "\n",
    "# TODO 5: decide your audit columns; e.g. ingest_ts, batch_id, process_id…\n",
    "df_bronze = df_raw \\\n",
    "  .withColumn(\"ingest_ts\", current_timestamp()) \\\n",
    "  .withColumn(\"batch_id\",   lit(\"TODO_YOUR_BATCH_ID\"))\n",
    "\n",
    "# TODO 6: choose your write mode: append vs overwrite; partitioning?\n",
    "df_bronze.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .save(bronze_path)\n",
    "\n",
    "# sanity-check\n",
    "display(spark.read.format(\"delta\").load(bronze_path))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
