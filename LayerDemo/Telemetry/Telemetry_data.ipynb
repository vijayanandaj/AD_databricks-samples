{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bee0bc9b-cffe-4922-bda2-d396f1b7d7bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create Temlemetry Data\n",
    "import uuid\n",
    "\n",
    "process_id = f\"demo_ingest|{uuid.uuid4()}\"\n",
    "\n",
    "# 1) Imports & parameters\n",
    "from pyspark.sql.functions import (\n",
    "    rand, floor, concat, lit,\n",
    "    current_timestamp, unix_timestamp,\n",
    "    when, col, struct, array, element_at\n",
    ")\n",
    "\n",
    "num_unique = 50_000\n",
    "apps    = [\"Photos\",\"Mail\",\"Chat\",\"Store\",\"Search\"]\n",
    "events  = [\"click\",\"view\",\"purchase\",\"login\",\"logout\",\"scroll\"]\n",
    "oss     = [\"iOS\",\"Android\",\"Windows\",\"Linux\"]\n",
    "regions = [\"IN\",\"US\",\"EU\",\"APAC\",\"LATAM\"]\n",
    "\n",
    "# 2) Build 50k unique events\n",
    "base = (\n",
    "    spark.range(num_unique)\n",
    "         .withColumn(\"user_id\", concat(lit(\"user_\"), floor(rand()*10000).cast(\"int\")))\n",
    "         .withColumn(\"app_name\",\n",
    "             element_at(array(*[lit(a) for a in apps]),\n",
    "                        (floor(rand()*len(apps)) + 1).cast(\"int\"))\n",
    "         )\n",
    "         .withColumn(\"event_type\",\n",
    "             element_at(array(*[lit(e) for e in events]),\n",
    "                        (floor(rand()*len(events)) + 1).cast(\"int\"))\n",
    "         )\n",
    "         .withColumn(\"event_timestamp\",\n",
    "             (unix_timestamp() - floor(rand()*86400)).cast(\"timestamp\")\n",
    "         )\n",
    "         .withColumn(\"device\",\n",
    "             struct(\n",
    "               element_at(array(*[lit(o) for o in oss]),\n",
    "                          (floor(rand()*len(oss)) + 1).cast(\"int\")).alias(\"os\"),\n",
    "               element_at(array(*[lit(r) for r in regions]),\n",
    "                          (floor(rand()*len(regions)) + 1).cast(\"int\")).alias(\"region\")\n",
    "             )\n",
    "         )\n",
    ")\n",
    "\n",
    "# 3) Inject ~1% nulls / ~1% garbage\n",
    "base = (\n",
    "    base\n",
    "      .withColumn(\"user_id\",\n",
    "          when(rand() < 0.01, None).otherwise(col(\"user_id\"))\n",
    "      )\n",
    "      .withColumn(\"event_type\",\n",
    "          when(rand() < 0.01, lit(\"BAD_EVENT\")).otherwise(col(\"event_type\"))\n",
    "      )\n",
    ")\n",
    "\n",
    "# 4) Duplicate to reach 100k total\n",
    "telemetry1 = base.union(base)\n",
    "\n",
    "# 5) Add audit columns & write out as JSON\n",
    "telemetry1 = telemetry1 \\\n",
    "    .withColumn(\"ingest_ts\",  current_timestamp()) \\\n",
    "    .withColumn(\"process_id\", lit(process_id))\n",
    "\n",
    "dbutils.fs.rm(\"dbfs:/tmp/raw/telemetry1/\", recurse=True)\n",
    "telemetry1.write.mode(\"overwrite\").json(\"dbfs:/tmp/raw/telemetry1/\")\n",
    "\n",
    "# 6) Sanityâ€check\n",
    "print(\"Total events written:\",\n",
    "      spark.read.json(\"dbfs:/tmp/raw/telemetry1/\").count())\n",
    "spark.read.json(\"dbfs:/tmp/raw/telemetry1/\").show(5, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Telemetry_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
