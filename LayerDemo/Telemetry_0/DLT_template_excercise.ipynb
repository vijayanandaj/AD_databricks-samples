{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b6976674-222b-4e4d-8dcf-a4b8b6edd782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Bronze: just land raw data into Delta\n",
    "# -----------------------------------------------------------------------------\n",
    "@dlt.table(\n",
    "    name=\"bronze_events\",\n",
    "    comment=\"Raw ingested data; exactly the files you pointed at in 01_Bronze.py\"\n",
    ")\n",
    "def bronze_events():\n",
    "    # Copy your spark.read() + write code from 01_Bronze.py here…\n",
    "    df = (\n",
    "        spark.read\n",
    "             .format(\"json\")           # or parquet — whatever your sample does\n",
    "             .load(\"/mnt/raw/source_path\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Silver: flatten, dedupe, conform, etc.\n",
    "# -----------------------------------------------------------------------------\n",
    "@dlt.table(\n",
    "    name=\"silver_events\",\n",
    "    comment=\"Flattened structs, deduplicated, with feature_category joined on\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_event_ts\", \"event_ts IS NOT NULL\")   # example expectation\n",
    "def silver_events():\n",
    "    bronze = dlt.read(\"bronze_events\")\n",
    "\n",
    "    # 2.a) flatten nested structs (if any)\n",
    "    # Copy your flatten logic: e.g.\n",
    "    flat = (\n",
    "        bronze\n",
    "        .select(\n",
    "            col(\"user_id\"),\n",
    "            col(\"event.app_name\").alias(\"app_name\"),\n",
    "            col(\"event.event_type\").alias(\"event_type\"),\n",
    "            col(\"event.event_ts\").alias(\"event_ts\"),\n",
    "            *[c for c in bronze.columns if c not in (\"event\",)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2.b) deduplicate on natural key\n",
    "    deduped = flat.dropDuplicates(\n",
    "        [\"user_id\",\"app_name\",\"event_type\",\"event_ts\"]\n",
    "    )\n",
    "\n",
    "    # 2.c) conformance join against small lookup table\n",
    "    feature_cat = (\n",
    "        spark.table(\"feature_catalog\")  # or read the small lookup from Bronze\n",
    "    )\n",
    "    silver = (\n",
    "        deduped\n",
    "        .join(\n",
    "            feature_cat,\n",
    "            on=[\"app_name\",\"event_type\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return silver\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Gold: aggregates, roll-ups, business-level tables\n",
    "# -----------------------------------------------------------------------------\n",
    "@dlt.table(\n",
    "    name=\"gold_event_summary\",\n",
    "    comment=\"Business-level summary of events per user/app/…\"\n",
    ")\n",
    "def gold_event_summary():\n",
    "    silver = dlt.read(\"silver_events\")\n",
    "\n",
    "    gold = (\n",
    "        silver\n",
    "        .groupBy(\"user_id\", \"feature_category\")\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"event_count\"),\n",
    "            min(\"event_ts\").alias(\"first_seen\"),\n",
    "            max(\"event_ts\").alias(\"last_seen\")\n",
    "        )\n",
    "    )\n",
    "    return gold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2241408e-9228-4f59-9825-24cfe29e08f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# medallion_dlt.py\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql.functions import (\n",
    "    current_timestamp, lit,\n",
    "    to_timestamp, to_date,\n",
    "    col, count, countDistinct\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 0) Optional: define your Bronze schema so you never get raw strings\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "bronze_schema = StructType([\n",
    "    StructField(\"user_id\",         StringType(), True),\n",
    "    StructField(\"app_name\",        StringType(), True),\n",
    "    StructField(\"event_type\",      StringType(), True),\n",
    "    StructField(\"event_timestamp\", StringType(), True),\n",
    "    StructField(\"device\", StructType([\n",
    "        StructField(\"os\",     StringType(), True),\n",
    "        StructField(\"region\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) Bronze: land raw JSON + audit columns\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "@dlt.table(\n",
    "    name=\"bronze_cc_events\",\n",
    "    comment=\"Raw CreativeCloud telemetry with ingest_ts & process_id\",\n",
    "    table_properties={\n",
    "        \"pipelines.trigger\": \"continuous\"\n",
    "    }\n",
    ")\n",
    "def bronze_cc_events():\n",
    "    df = (\n",
    "        spark.read\n",
    "             .schema(bronze_schema)\n",
    "             .option(\"multiline\", True)\n",
    "             .json(\"/tmp/raw/telemetry\")\n",
    "    )\n",
    "    return (\n",
    "        df\n",
    "          .withColumn(\"ingest_ts\",  current_timestamp())\n",
    "          .withColumn(\"process_id\", lit(dlt.current_timestamp().cast(\"string\")))\n",
    "    )\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Silver lookup: small reference table for conformance\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "@dlt.table(\n",
    "    name=\"cc_feature_ref\",\n",
    "    comment=\"app+event_type → feature_category lookup\",\n",
    "    table_properties={\"pipelines.trigger\": \"once\"}\n",
    ")\n",
    "def cc_feature_ref():\n",
    "    return (\n",
    "        spark.read\n",
    "             .option(\"header\", True)\n",
    "             .csv(\"/tmp/reference/cc_features.csv\")\n",
    "    )\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Silver: clean, flatten, dedupe & conformance join\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "@dlt.table(\n",
    "    name=\"silver_cc_events\",\n",
    "    comment=\"Flattened, cleaned & conformed events\",\n",
    "    table_properties={\"pipelines.trigger\": \"continuous\"}\n",
    ")\n",
    "@dlt.expect(\"valid_event_ts\", \"event_ts IS NOT NULL\")\n",
    "def silver_cc_events():\n",
    "    bronze = dlt.read(\"bronze_cc_events\")\n",
    "\n",
    "    # flatten + cast + filter + dedupe\n",
    "    silver = (\n",
    "        bronze\n",
    "          .filter(col(\"user_id\").isNotNull() & col(\"event_type\").isNotNull())\n",
    "          .withColumn(\"event_ts\",   to_timestamp(\"event_timestamp\",\"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
    "          .withColumn(\"event_date\", to_date(\"event_ts\"))\n",
    "          .withColumn(\"os\",         col(\"device.os\"))\n",
    "          .withColumn(\"region\",     col(\"device.region\"))\n",
    "          .select(\n",
    "             \"user_id\",\"app_name\",\"event_type\",\n",
    "             \"event_ts\",\"event_date\",\"os\",\"region\",\n",
    "             \"ingest_ts\",\"process_id\"\n",
    "          )\n",
    "          .dropDuplicates([\"user_id\",\"app_name\",\"event_type\",\"event_ts\"])\n",
    "    )\n",
    "\n",
    "    # conformance join\n",
    "    return silver.join(\n",
    "        dlt.read(\"cc_feature_ref\"),\n",
    "        on=[\"app_name\",\"event_type\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4) Gold fact: daily feature usage\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "@dlt.table(\n",
    "    name=\"feature_usage_fact\",\n",
    "    comment=\"Daily counts of feature usage\",\n",
    "    table_properties={\"pipelines.trigger\": \"continuous\"}\n",
    ")\n",
    "def feature_usage_fact():\n",
    "    silver = dlt.read(\"silver_cc_events\")\n",
    "    return (\n",
    "        silver\n",
    "          .filter(col(\"feature_category\").isNotNull())\n",
    "          .groupBy(\"event_date\",\"app_name\",\"feature_category\")\n",
    "          .agg(count(\"*\").alias(\"usage_count\"))\n",
    "    )\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5) Gold fact: daily active users\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "@dlt.table(\n",
    "    name=\"user_activity_fact\",\n",
    "    comment=\"Daily unique active users by app/region\",\n",
    "    table_properties={\"pipelines.trigger\": \"continuous\"}\n",
    ")\n",
    "def user_activity_fact():\n",
    "    silver = dlt.read(\"silver_cc_events\")\n",
    "    return (\n",
    "        silver\n",
    "          .groupBy(\"event_date\",\"app_name\",\"region\")\n",
    "          .agg(countDistinct(\"user_id\").alias(\"active_users\"))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT_template_excercise",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
