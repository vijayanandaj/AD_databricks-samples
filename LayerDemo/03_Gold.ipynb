{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e279a208-647f-406c-ab14-805ede20c752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#03_Gold layer code \n",
    "# ───────────────────────────────────────────────────────────\n",
    "# Gold Layer: Curated, Consumption-Ready Fact Tables\n",
    "# ───────────────────────────────────────────────────────────\n",
    "\n",
    "from pyspark.sql.functions import count, countDistinct\n",
    "\n",
    "# 1) Define paths\n",
    "silver_path           = \"dbfs:/tmp/silver/cc_events_enterprise/\"\n",
    "feature_usage_gold    = \"dbfs:/tmp/gold/feature_usage_fact/\"\n",
    "user_activity_gold    = \"dbfs:/tmp/gold/user_activity_fact/\"\n",
    "\n",
    "# 2) Read the Silver “enterprise view”\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "display(silver_df.limit(5))\n",
    "\n",
    "# 3) Feature Usage Fact\n",
    "feature_usage = (\n",
    "    silver_df\n",
    "      .filter(\"feature_category IS NOT NULL\")                  # only track known features\n",
    "      .groupBy(\"event_date\", \"app_name\", \"feature_category\")   # natural grain\n",
    "      .agg(count(\"*\").alias(\"usage_count\"))                    # daily usage per feature\n",
    ")\n",
    "\n",
    "# 4) Write Feature Usage to Delta, partitioned by event_date\n",
    "feature_usage.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .save(feature_usage_gold)\n",
    "\n",
    "display(spark.read.format(\"delta\").load(feature_usage_gold))\n",
    "\n",
    "# 5) User Activity Fact\n",
    "user_activity = (\n",
    "    silver_df\n",
    "      .groupBy(\"event_date\", \"app_name\", \"region\")\n",
    "      .agg(countDistinct(\"user_id\").alias(\"active_users\"))     # daily unique users\n",
    ")\n",
    "\n",
    "# 6) Write User Activity to Delta, partitioned by event_date\n",
    "user_activity.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\") \\\n",
    "    .save(user_activity_gold)\n",
    "\n",
    "display(spark.read.format(\"delta\").load(user_activity_gold))\n",
    "\n",
    "# 7) Optimize Gold tables for speed (Z-Order on high-cardinality column)\n",
    "spark.sql(f\"OPTIMIZE delta.`{feature_usage_gold}` ZORDER BY (feature_category)\")\n",
    "spark.sql(f\"OPTIMIZE delta.`{user_activity_gold}`   ZORDER BY (region)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
